{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>pose_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>frame</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>1003</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>1004</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12105</th>\n",
       "      <td>45160</td>\n",
       "      <td>145160</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12106</th>\n",
       "      <td>45161</td>\n",
       "      <td>145161</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12107</th>\n",
       "      <td>45162</td>\n",
       "      <td>145162</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12108</th>\n",
       "      <td>45163</td>\n",
       "      <td>145163</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12109</th>\n",
       "      <td>45164</td>\n",
       "      <td>145164</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12110 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  pose_id                                         file_name  \\\n",
       "0          1000     1000               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "1          1001     1001               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "2          1002     1002               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "3          1003     1003               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "4          1004     1004               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "...         ...      ...                                               ...   \n",
       "12105     45160   145160  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12106     45161   145161  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12107     45162   145162  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12108     45163   145163  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12109     45164   145164  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "\n",
       "       frame  person  \n",
       "0          0       0  \n",
       "1          1       0  \n",
       "2          2       0  \n",
       "3          3       0  \n",
       "4          4       0  \n",
       "...      ...     ...  \n",
       "12105    160       1  \n",
       "12106    161       1  \n",
       "12107    162       1  \n",
       "12108    163       1  \n",
       "12109    164       1  \n",
       "\n",
       "[12110 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "\n",
    "from base64 import b64encode\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from IPython.display import HTML\n",
    "from cotracker.predictor import CoTrackerPredictor, CoTrackerOnlinePredictor\n",
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "# model = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker2_online\").to(\"cuda\")\n",
    "DEFAULT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoTrackerPredictor(checkpoint=os.path.join('./co-tracker/checkpoints/cotracker2.pth'))#.to(DEFAULT_DEVICE)\n",
    "\n",
    "NUM_KEYPOINTS = 23#26\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "df_running_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_step(window_frames, is_first_step, queries = None):\n",
    "        video_chunk = (\n",
    "            torch.tensor(np.stack(window_frames[-model.step * 2 :]), device=DEFAULT_DEVICE)\n",
    "            .float()\n",
    "            .permute(0, 3, 1, 2)[None]\n",
    "        )  # (1, T, 3, H, W)\n",
    "        if queries is None or len(queries) == 0:\n",
    "            return model(\n",
    "            video_chunk,\n",
    "            is_first_step=is_first_step\n",
    "            )\n",
    "        else:\n",
    "            if torch.cuda.is_available():\n",
    "                queries = queries.cuda()\n",
    "                \n",
    "            return model(\n",
    "                video_chunk,\n",
    "                is_first_step=is_first_step,\n",
    "                queries=queries[None]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_video(input_path, output_path, x_start, y_start, width, height):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    # Get the original video dimensions\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can change the codec based on your requirements\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
    "\n",
    "    # Iterate through the frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if we reach the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Crop the frame to the specified dimensions\n",
    "        cropped_frame = frame[y_start:y_start + height, x_start:x_start + width]\n",
    "\n",
    "        # Resize the cropped frame to the new dimensions\n",
    "        resized_frame = cv2.resize(cropped_frame, (width, height))\n",
    "\n",
    "        # Write the resized frame to the output video file\n",
    "        out.write(resized_frame)\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Close any open windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import peakutils\n",
    "from KeyFrameDetector.utils import convert_frame_to_grayscale\n",
    "\n",
    "def get_keyframes(filename):\n",
    "    cap = cv2.VideoCapture('./data/videos/' + filename)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    lstfrm = []\n",
    "    lstdiffMag = []\n",
    "    timeSpans = []\n",
    "    images = []\n",
    "    full_color = []\n",
    "    lastFrame = None\n",
    "\n",
    "    for i in range(length):\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        grayframe, blur_gray = convert_frame_to_grayscale(frame)\n",
    "\n",
    "        frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "        lstfrm.append(frame_number)\n",
    "        images.append(grayframe)\n",
    "        full_color.append(frame)\n",
    "        if frame_number == 0:\n",
    "            lastFrame = blur_gray\n",
    "\n",
    "        diff = cv2.subtract(blur_gray, lastFrame)\n",
    "        diffMag = cv2.countNonZero(diff)\n",
    "        lstdiffMag.append(diffMag)\n",
    "        lastFrame = blur_gray\n",
    "\n",
    "    \n",
    "    y = np.array(lstdiffMag)\n",
    "    base = peakutils.baseline(y, 2)\n",
    "    indices = peakutils.indexes(y-base, float(0.3), min_dist=1)\n",
    "\n",
    "    # reset to first frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def cotracker_model(video, filename, person_count, queries_dict, indices, id, visualize = True):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "\n",
    "    queries = queries_dict[0]\n",
    "    idx = []\n",
    "    #indices = [0]\n",
    "    for i, ind in enumerate(indices):\n",
    "        if ind == 0:\n",
    "            continue\n",
    "        if queries_dict[ind] is None or len(queries_dict[ind]) == 0:\n",
    "            idx.append(i)\n",
    "        #elif ind < len(queries_dict):\n",
    "        else:\n",
    "            queries = torch.vstack((queries, queries_dict[ind]))\n",
    "    indices = np.delete(indices, idx, axis=0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        queries = queries.cuda()\n",
    "        \n",
    "    pred_tracks, pred_visibility = model(\n",
    "                video,\n",
    "                queries=queries[None]\n",
    "            )\n",
    "    \n",
    "    new_pred_tracks = None    \n",
    "    new_pred_visibility = None    \n",
    "    keyframe_i = 0\n",
    "    total_kpts_count=NUM_KEYPOINTS*person_count\n",
    "\n",
    "    #indices = np.insert(indices, 0, 0)\n",
    "    for i in range(len(pred_tracks[0])):\n",
    "        frame = pred_tracks[0][i]\n",
    "        frame_vis = pred_visibility[0][i]\n",
    "\n",
    "        j = keyframe_i*total_kpts_count\n",
    "        k = (keyframe_i+1)*total_kpts_count\n",
    "        if k > frame.shape[0]:\n",
    "            k = frame.shape[0]-1\n",
    "        new_frame = frame[j:k]\n",
    "        new_frame_vis = frame_vis[j:k]\n",
    "        \n",
    "        if new_frame.shape[0] < total_kpts_count:\n",
    "            new_pred_tracks = torch.cat((new_pred_tracks, frame.unsqueeze(0)),dim=0)\n",
    "            new_pred_visibility = torch.cat((new_pred_visibility, frame_vis.unsqueeze(0)),dim=0)\n",
    "            continue\n",
    "\n",
    "        if new_pred_tracks is None:\n",
    "            new_pred_tracks = new_frame\n",
    "            new_pred_visibility = new_frame_vis\n",
    "        elif len(new_pred_tracks.shape) == 2:\n",
    "            new_pred_tracks = torch.stack((new_pred_tracks, new_frame),dim=0)\n",
    "            new_pred_visibility = torch.stack((new_pred_visibility, new_frame_vis),dim=0)\n",
    "        else:\n",
    "            new_pred_tracks = torch.cat((new_pred_tracks, new_frame.unsqueeze(0)),dim=0)\n",
    "            new_pred_visibility = torch.cat((new_pred_visibility, new_frame_vis.unsqueeze(0)),dim=0)\n",
    "            \n",
    "        if keyframe_i+1 < len(indices) and indices[keyframe_i+1] <= i+1:\n",
    "            keyframe_i += 1\n",
    "            \n",
    "    if visualize:\n",
    "        vis = Visualizer(save_dir=\"./videos/results/cotracker3/\", linewidth=3)\n",
    "        vis.visualize(video, torch.unsqueeze(new_pred_tracks, 0), torch.unsqueeze(new_pred_visibility, 0), filename=filename.replace('.mp4', '') + '_' + str(id), query_frame=0)\n",
    "    \n",
    "    return torch.unsqueeze(new_pred_tracks, 0), torch.unsqueeze(new_pred_visibility, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Athletics_Mixed_Tokyo_2020_20_1.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_39.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_5.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_64.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_8.mp4', 'clip_tri_1.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_14.mp4', 'Marathon_Men_Tokyo_2020_16.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_28.mp4', 'Marathon_Men_Tokyo_2020_47.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_52.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_12_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_19_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_21_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_23.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_28.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_29.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_2_1.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_33_1.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_23.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_25.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_28.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_38.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_46.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_49.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_18.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_20.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_21.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_26.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_3.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_9.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_10.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_32.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_35.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_43.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_46.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_5.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_24.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_26.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_28.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_7.mp4',\n",
       "       'World_Athletics_Women_Marathon_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Women_Marathon_Oregon_2022_8.mp4'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "filenames = df_running_annotations['file_name'].unique()\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CoTrackerPredictor and create an instance of it. We'll use this object to estimate tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_video(filename):\n",
    "    video = read_video_from_path('./data/videos/' + filename)\n",
    "    video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        video = video.cuda()\n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking manually selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, json, re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    frames = defaultdict(list)\n",
    "    isExist = os.path.exists(file_path)\n",
    "    person_count = 0\n",
    "    if isExist:\n",
    "      annotations = []\n",
    "      with open(file_path, 'r') as f:\n",
    "          annotations = json.load(f)\n",
    "          \n",
    "          keypoints = defaultdict(list)\n",
    "          person_count = len(annotations['annotations'])\n",
    "          for person in annotations['annotations']:\n",
    "            framecount = len(person['frames'])\n",
    "            for frame_index in range(0, framecount):\n",
    "              points = []\n",
    "              if frame_index < framecount:\n",
    "                frame = person['frames'][str(frame_index)]\n",
    "                for node in frame['skeleton']['nodes']:\n",
    "                    points.append({'id': node['name'], 'x' : node['x'], 'y': node['y']})\n",
    "\n",
    "              if len(points) > 0:\n",
    "                keypoints[frame_index].append({'person': {'points': points}})\n",
    "\n",
    "          for frame in range(0, len(keypoints)):\n",
    "            frames[frame] = keypoints[frame]\n",
    "\n",
    "    return frames, person_count\n",
    "\n",
    "\n",
    "def get_queries_for_frame(frame_number, annotations):\n",
    "  i = 0\n",
    "\n",
    "  for person in annotations[frame_number]:\n",
    "    for point in person['person']['points']:\n",
    "      new_tensor = torch.tensor([float(frame_number), point['x'], point['y']])\n",
    "      if i == 0:\n",
    "        queries_for_frames = new_tensor\n",
    "      else:\n",
    "        queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "      i += 1\n",
    "\n",
    "  return queries_for_frames\n",
    "\n",
    "\n",
    "def get_queries_for_frames(start_frame, end_frame, annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(annotations)\n",
    "\n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    for person in annotations[frame]:\n",
    "      for point in person['person']['points']:\n",
    "        if 'occluded' in point and point['occluded'] == 'false':\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), point['x'], point['y']])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict\n",
    "\n",
    "def get_bboxs(df, df_annotations):\n",
    "  if df_annotations is not None and len(df_annotations) > 0:\n",
    "    # Find closest matching pose for first frame     \n",
    "    #gts = df_annotations[df_annotations['image_id'] == df['image_id']]\n",
    "    id_box = defaultdict(list)\n",
    "    idxs = []\n",
    "    first_frame = df_annotations[df_annotations['image_id'] == df_annotations['image_id'].iloc[0]]\n",
    "\n",
    "    for pose_id in first_frame.index:\n",
    "      poses = df[df['image_id'] == df_annotations['image_id'][pose_id]]\n",
    "      min_dist = np.inf\n",
    "      gt = df_annotations['keypoints'][pose_id]\n",
    "      kpts2, vi2 = customutils.edit_keypoints(gt)\n",
    "\n",
    "      # compute head size for distance normalization\n",
    "      head = customutils.get_keypoint(gt,\"head\")\n",
    "      neck = customutils.get_keypoint(gt,\"neck\")\n",
    "\n",
    "      headSize = 1\n",
    "      if (len(head) > 0 and len(neck) > 0):\n",
    "          headSize = customutils.get_head_size(head[0], head[1], neck[0], neck[1])\n",
    "          \n",
    "      for pose in poses.iloc:\n",
    "        # Threshold to consider valid pose\n",
    "        if pose['score'] > .8:\n",
    "          dt = pose['keypoints']\n",
    "          kpts1, vi1 = customutils.edit_keypoints(dt)\n",
    "          d = np.linalg.norm(kpts1 - kpts2, ord=2, axis=1)\n",
    "          v = np.ones(len(d))\n",
    "\n",
    "          for part in range(len(d)):\n",
    "              if vi1[part] == 0 or vi2[part] == 0:\n",
    "                  d[part] = 0\n",
    "                  v[part] = 0\n",
    "\n",
    "          # normalize distance\n",
    "          dNorm = np.sum(d)/headSize\n",
    "\n",
    "          if dNorm < min_dist:\n",
    "            min_dist = dNorm\n",
    "            idx = pose['idx']\n",
    "\n",
    "      idxs.append(idx)\n",
    "    \n",
    "    # For each pose in tracking\n",
    "    for pose in df.iloc:\n",
    "      if pose['idx'] not in idxs:\n",
    "        continue\n",
    "\n",
    "      frame = pose['image_id'] % 1000\n",
    "      idx = pose['idx']\n",
    "      bbox = np.array(customutils.get_bbox_coord(pose['box']))\n",
    "      if id_box[idx] is None or len(id_box[idx]) == 0:\n",
    "        id_box[idx] = defaultdict(list)\n",
    "        id_box[idx][frame] = bbox\n",
    "      else:\n",
    "        id_box[idx][frame] = bbox\n",
    "\n",
    "  return idxs, id_box\n",
    "\n",
    "def get_queries_from_pe(start_frame, end_frame, indices, df, boxes):\n",
    "  frames_dict = defaultdict(list)\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(df)\n",
    "  \n",
    "  #for frame in range(start_frame, end_frame):\n",
    "  for frame in indices:\n",
    "    bbox = boxes[frame]\n",
    "    if bbox is None or len(bbox) == 0:\n",
    "      continue\n",
    "\n",
    "    queries_for_frames = None\n",
    "    poses = df[df['image_id'] % 1000 == frame]\n",
    "    min_dist = np.inf\n",
    "    selected_pose = None\n",
    "    for pose in poses.iloc:\n",
    "      box = np.array(pose['box'][:-1])\n",
    "      d = np.linalg.norm(bbox - box, ord=2)\n",
    "      d = np.sum(d)\n",
    "      if d < min_dist:\n",
    "        min_dist = d\n",
    "        selected_pose = pose\n",
    "\n",
    "    if selected_pose is not None:\n",
    "      kpts = selected_pose['keypoints']\n",
    "      x, y, vi = customutils.get_x_y_v_keypoints(kpts)\n",
    "      for point in range(len(x)):\n",
    "        #if vi[point] == 0:\n",
    "        if x[point] == float(0) or y[point] == float(0): #skip head, neck, hip\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), float(x[point]), float(y[point])])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return frames_dict\n",
    "\n",
    "def get_idxs(df, df_annotations):\n",
    "  if df_annotations is not None and len(df_annotations) > 0:\n",
    "    # Find closest matching pose for first frame \n",
    "    poses = df[df['frame'] == 0]\n",
    "    gts = df_annotations[df_annotations['image_id'] == poses['image_id'].iloc[0]]\n",
    "    idxs = []\n",
    "    \n",
    "    # For each pose in ground truth\n",
    "    for pose_id in gts.index:\n",
    "        min_dist = np.inf\n",
    "        gt = gts['keypoints'][pose_id]\n",
    "        kpts2, vi2 = customutils.edit_keypoints(gt)\n",
    "                                \n",
    "        area = customutils.compute_area_keypoints(gt)\n",
    "\n",
    "        # compute head size for distance normalization\n",
    "        head = customutils.get_keypoint(gt,\"head\")\n",
    "        neck = customutils.get_keypoint(gt,\"neck\")\n",
    "\n",
    "        headSize = 1\n",
    "        if (len(head) > 0 and len(neck) > 0):\n",
    "            headSize = customutils.get_head_size(head[0], head[1], neck[0], neck[1])\n",
    "            \n",
    "        for pose in poses.iloc:\n",
    "          # Threshold to consider valid pose\n",
    "          if pose['score'] > .8:\n",
    "            dt = pose['keypoints']\n",
    "            kpts1, vi1 = customutils.edit_keypoints(dt)\n",
    "            d = np.linalg.norm(kpts1 - kpts2, ord=2, axis=1)\n",
    "            v = np.ones(len(d))\n",
    "\n",
    "            for part in range(len(d)):\n",
    "                if vi1[part] == 0 or vi2[part] == 0:\n",
    "                    d[part] = 0\n",
    "                    v[part] = 0\n",
    "\n",
    "            # normalize distance\n",
    "            dNorm = np.sum(d)/headSize\n",
    "\n",
    "            if dNorm < min_dist:\n",
    "              min_dist = dNorm\n",
    "              idx = pose['idx']\n",
    "\n",
    "        # Add closest matching pose\n",
    "        idxs.append(idx)\n",
    "\n",
    "  return idxs\n",
    "\n",
    "def get_queries_from_pe_idx(start_frame, end_frame, df, idx):\n",
    "  frames_dict = defaultdict(list)\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(df)\n",
    "  \n",
    "  for frame in range(start_frame, end_frame):\n",
    "    queries_for_frames = None\n",
    "    poses = df[df['frame'] == frame]\n",
    "    for pose in poses.iloc:\n",
    "\n",
    "      if idx == -1:\n",
    "        idx = pose['idx']\n",
    "\n",
    "      if idx != pose['idx']:\n",
    "        continue\n",
    "\n",
    "      kpts = pose['keypoints']\n",
    "      x, y, vi = customutils.get_x_y_v_keypoints(kpts)\n",
    "      for point in range(len(x)):\n",
    "        if vi[point] == 0:\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), float(x[point]), float(y[point])])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "\n",
    "      break\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return frames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>area</th>\n",
       "      <th>iscrowd</th>\n",
       "      <th>num_keypoints</th>\n",
       "      <th>keypoints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>[957.5031, 204.6478, 294.4987, 641.9175]</td>\n",
       "      <td>189043.869257</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[1132.7201, 271.7583, 1, 1151.8834, 256.5806, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>[895.2875, 201.4572, 396.59609999999986, 639.6...</td>\n",
       "      <td>253699.828317</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[1135.4403, 262.4473, 1, 1151.8834, 253.39, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>[876.5417, 207.8383, 455.8919000000001, 641.2885]</td>\n",
       "      <td>292358.232713</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[1132.7201, 255.8056, 1, 1148.6928, 247.0089, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>[892.6316, 187.0998, 445.91919999999993, 658.8...</td>\n",
       "      <td>293787.800419</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[1129.5295, 249.4245, 1, 1145.5023, 240.6278, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>[912.8355, 201.4572, 406.572, 681.1703]</td>\n",
       "      <td>276944.771212</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[1132.7201, 249.4245, 1, 1148.6928, 240.6278, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12105</th>\n",
       "      <td>145160</td>\n",
       "      <td>45160</td>\n",
       "      <td>1</td>\n",
       "      <td>[749.0932, 271.5931, 241.01109999999994, 816.5...</td>\n",
       "      <td>196795.299998</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12106</th>\n",
       "      <td>145161</td>\n",
       "      <td>45161</td>\n",
       "      <td>1</td>\n",
       "      <td>[749.0932, 271.5931, 241.01109999999994, 816.5...</td>\n",
       "      <td>196795.299998</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12107</th>\n",
       "      <td>145162</td>\n",
       "      <td>45162</td>\n",
       "      <td>1</td>\n",
       "      <td>[749.0932, 271.5931, 241.01109999999994, 816.5...</td>\n",
       "      <td>196795.299998</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12108</th>\n",
       "      <td>145163</td>\n",
       "      <td>45163</td>\n",
       "      <td>1</td>\n",
       "      <td>[749.0932, 271.5931, 241.01109999999994, 816.5...</td>\n",
       "      <td>196795.299998</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12109</th>\n",
       "      <td>145164</td>\n",
       "      <td>45164</td>\n",
       "      <td>1</td>\n",
       "      <td>[752.4083, 271.5931, 242.11609999999996, 816.5...</td>\n",
       "      <td>197697.577140</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>[811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12110 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  image_id  category_id  \\\n",
       "0        1000      1000            1   \n",
       "1        1001      1001            1   \n",
       "2        1002      1002            1   \n",
       "3        1003      1003            1   \n",
       "4        1004      1004            1   \n",
       "...       ...       ...          ...   \n",
       "12105  145160     45160            1   \n",
       "12106  145161     45161            1   \n",
       "12107  145162     45162            1   \n",
       "12108  145163     45163            1   \n",
       "12109  145164     45164            1   \n",
       "\n",
       "                                                    bbox           area  \\\n",
       "0               [957.5031, 204.6478, 294.4987, 641.9175]  189043.869257   \n",
       "1      [895.2875, 201.4572, 396.59609999999986, 639.6...  253699.828317   \n",
       "2      [876.5417, 207.8383, 455.8919000000001, 641.2885]  292358.232713   \n",
       "3      [892.6316, 187.0998, 445.91919999999993, 658.8...  293787.800419   \n",
       "4                [912.8355, 201.4572, 406.572, 681.1703]  276944.771212   \n",
       "...                                                  ...            ...   \n",
       "12105  [749.0932, 271.5931, 241.01109999999994, 816.5...  196795.299998   \n",
       "12106  [749.0932, 271.5931, 241.01109999999994, 816.5...  196795.299998   \n",
       "12107  [749.0932, 271.5931, 241.01109999999994, 816.5...  196795.299998   \n",
       "12108  [749.0932, 271.5931, 241.01109999999994, 816.5...  196795.299998   \n",
       "12109  [752.4083, 271.5931, 242.11609999999996, 816.5...  197697.577140   \n",
       "\n",
       "       iscrowd  num_keypoints  \\\n",
       "0            1             26   \n",
       "1            1             26   \n",
       "2            1             26   \n",
       "3            1             26   \n",
       "4            1             26   \n",
       "...        ...            ...   \n",
       "12105        1             26   \n",
       "12106        1             26   \n",
       "12107        1             26   \n",
       "12108        1             26   \n",
       "12109        1             26   \n",
       "\n",
       "                                               keypoints  \n",
       "0      [1132.7201, 271.7583, 1, 1151.8834, 256.5806, ...  \n",
       "1      [1135.4403, 262.4473, 1, 1151.8834, 253.39, 1,...  \n",
       "2      [1132.7201, 255.8056, 1, 1148.6928, 247.0089, ...  \n",
       "3      [1129.5295, 249.4245, 1, 1145.5023, 240.6278, ...  \n",
       "4      [1132.7201, 249.4245, 1, 1148.6928, 240.6278, ...  \n",
       "...                                                  ...  \n",
       "12105  [811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...  \n",
       "12106  [811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...  \n",
       "12107  [811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...  \n",
       "12108  [811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...  \n",
       "12109  [811.8716, 319.3694, 1, 827.6156, 303.8024, 1,...  \n",
       "\n",
       "[12110 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = customutils.load_keypoints_dataframe()\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(filename):\n",
    "    mask_file = df_running_annotations['file_name'] == filename\n",
    "    mask_frame = df_running_annotations['frame'] == 0\n",
    "    image_id = df_running_annotations.loc[mask_file & mask_frame]['image_id'].iloc[0]\n",
    "    return image_id\n",
    "    \n",
    "def run_cotracker(filename, video, df_det, df_pe):\n",
    "    frames = []\n",
    "    annot_json = 'videos/annotations/' + filename.replace('.mp4', '') + '.json'\n",
    "    annotations, person_count = load_annotations(annot_json)\n",
    "    df_video_imageids = df_running_annotations[df_running_annotations['file_name'] == filename]\n",
    "    \n",
    "    df_gt = df_annotations[df_annotations['image_id'].isin(df_video_imageids['image_id'])]\n",
    "    df = df_det[df_det['image_id'].isin(df_video_imageids['image_id'])]\n",
    "    idxs, idx_boxes = get_bboxs(df, df_gt)\n",
    "\n",
    "    indices = get_keyframes(filename)\n",
    "    indices = np.insert(indices, 0, 0)\n",
    "    df = df_pe[df_pe['image_id'].isin(df_video_imageids['image_id'])]\n",
    "\n",
    "    for idx in idxs:\n",
    "        queries_dict = get_queries_from_pe(0, -1, indices, df, idx_boxes[idx])\n",
    "        if len(queries_dict) == 0:\n",
    "            print('No queries found for ' + str(idx))\n",
    "            continue\n",
    "        print('Processing ' + str(idx))\n",
    "        pred_tracks, pred_visibility = cotracker_model(video, filename, 1, queries_dict, indices, idx)\n",
    "\n",
    "        num_frames = pred_tracks.cpu().shape[1]\n",
    "        np_pred = pred_tracks.cpu().numpy()\n",
    "        np_vis = pred_visibility.cpu().numpy()\n",
    "        #df = pd.DataFrame(index=range(num_frames), columns=[str(x) for x in range(num_frames)])\n",
    "        for i in range(num_frames):\n",
    "            frame = {}\n",
    "            frame['image_id'] = int(get_image_id(filename)) + i\n",
    "            frame['idx'] = int(idx)\n",
    "            frame['category_id'] = 1\n",
    "\n",
    "            keypoints = []\n",
    "            for j in range(len(np_pred[0][i])):\n",
    "                if j % NUM_KEYPOINTS == NUM_KEYPOINTS:\n",
    "                    # save current set of keypoints and start new frame/pose\n",
    "                    frame['keypoints'] = keypoints\n",
    "                    frames.append(frame)\n",
    "                    keypoints = []\n",
    "                keypoints.append(np_pred[0][i][j][0].astype(float))\n",
    "                keypoints.append(np_pred[0][i][j][1].astype(float))\n",
    "                if np_vis[0][i][j]:\n",
    "                    keypoints.append(2)\n",
    "                else:\n",
    "                    keypoints.append(1)\n",
    "\n",
    "            frame['keypoints'] = keypoints\n",
    "            frames.append(frame)\n",
    "\n",
    "    customutils.writeJson({'frames': frames, 'keyframes': indices.tolist()},'videos/results/cotracker3/' + filename.replace('.mp4', '') + '.json')\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>keypoints</th>\n",
       "      <th>box</th>\n",
       "      <th>idx</th>\n",
       "      <th>score</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>[672.5870361328125, 397.4309997558594, 2, 692....</td>\n",
       "      <td>[455.3162841796875, 340.884765625, 402.0437011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>[1136.07421875, 266.266845703125, 2, 1155.6085...</td>\n",
       "      <td>[932.1016845703125, 190.0831756591797, 310.273...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.877560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>[682.8905029296875, 407.08642578125, 2, 702.61...</td>\n",
       "      <td>[456.44172066815923, 337.55727539062497, 405.4...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>[1134.0335693359375, 257.7659912109375, 2, 115...</td>\n",
       "      <td>[938.4007471361167, 194.7648895263672, 306.255...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.867543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>[691.739990234375, 412.57452392578125, 2, 713....</td>\n",
       "      <td>[443.5269343344591, 336.3762005969505, 431.860...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.859866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45470</th>\n",
       "      <td>45163</td>\n",
       "      <td>[704.8864135742188, 428.4450988769531, 2, 708....</td>\n",
       "      <td>[604.3679742712545, 384.44794226840435, 129.43...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.775203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45471</th>\n",
       "      <td>45163</td>\n",
       "      <td>[242.5023956298828, 475.6257019042969, 2, 245....</td>\n",
       "      <td>[219.46201027790286, 441.22925448208105, 108.8...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.555517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45472</th>\n",
       "      <td>45163</td>\n",
       "      <td>[1689.3052978515625, 480.3915100097656, 2, 168...</td>\n",
       "      <td>[1669.4820232953628, 467.83948263581397, 20.96...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.275808</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45473</th>\n",
       "      <td>45163</td>\n",
       "      <td>[1490.57373046875, 472.3777160644531, 2, 1493....</td>\n",
       "      <td>[1481.6382385788675, 450.34171338820823, 32.94...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.676191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45474</th>\n",
       "      <td>45163</td>\n",
       "      <td>[1005.9852294921875, 454.8758239746094, 2, 100...</td>\n",
       "      <td>[956.6008190511593, 409.1107207819066, 84.5862...</td>\n",
       "      <td>63</td>\n",
       "      <td>0.826677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45475 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                          keypoints  \\\n",
       "0          1000  [672.5870361328125, 397.4309997558594, 2, 692....   \n",
       "1          1000  [1136.07421875, 266.266845703125, 2, 1155.6085...   \n",
       "2          1001  [682.8905029296875, 407.08642578125, 2, 702.61...   \n",
       "3          1001  [1134.0335693359375, 257.7659912109375, 2, 115...   \n",
       "4          1002  [691.739990234375, 412.57452392578125, 2, 713....   \n",
       "...         ...                                                ...   \n",
       "45470     45163  [704.8864135742188, 428.4450988769531, 2, 708....   \n",
       "45471     45163  [242.5023956298828, 475.6257019042969, 2, 245....   \n",
       "45472     45163  [1689.3052978515625, 480.3915100097656, 2, 168...   \n",
       "45473     45163  [1490.57373046875, 472.3777160644531, 2, 1493....   \n",
       "45474     45163  [1005.9852294921875, 454.8758239746094, 2, 100...   \n",
       "\n",
       "                                                     box  idx     score  \\\n",
       "0      [455.3162841796875, 340.884765625, 402.0437011...    1  0.898572   \n",
       "1      [932.1016845703125, 190.0831756591797, 310.273...    2  0.877560   \n",
       "2      [456.44172066815923, 337.55727539062497, 405.4...    1  0.891666   \n",
       "3      [938.4007471361167, 194.7648895263672, 306.255...    2  0.867543   \n",
       "4      [443.5269343344591, 336.3762005969505, 431.860...    1  0.859866   \n",
       "...                                                  ...  ...       ...   \n",
       "45470  [604.3679742712545, 384.44794226840435, 129.43...   29  0.775203   \n",
       "45471  [219.46201027790286, 441.22925448208105, 108.8...   12  0.555517   \n",
       "45472  [1669.4820232953628, 467.83948263581397, 20.96...   37  0.275808   \n",
       "45473  [1481.6382385788675, 450.34171338820823, 32.94...   54  0.676191   \n",
       "45474  [956.6008190511593, 409.1107207819066, 84.5862...   63  0.826677   \n",
       "\n",
       "       category_id  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "45470            1  \n",
       "45471            1  \n",
       "45472            1  \n",
       "45473            1  \n",
       "45474            1  \n",
       "\n",
       "[45475 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_pe_alphapose = customutils.load_pe_dataframe('alphapose')\n",
    "df_pe_openpose = customutils.load_pe_dataframe('openpose')\n",
    "df_pe_ViTPose = customutils.load_pe_dataframe('ViTPose')\n",
    "\n",
    "df_pe_alphapose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Triathlon_Men_Tokyo_2020_28.mp4\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "for filename in filenames:\n",
    "    if filename == \"Marathon_Men_Tokyo_2020_14.mp4\" or filename == \"Triathlon_Men_Tokyo_2020_19_1.mp4\":\n",
    "        continue\n",
    "    # if filename == \"Triathlon_Men_Tokyo_2020_28.mp4\":\n",
    "        # continue\n",
    "    # if filename != \"Athletics_Mixed_Tokyo_2020_8.mp4\":\n",
    "        # continue\n",
    "    isExist = os.path.exists(\"./videos/results/cotracker3/\" + filename.replace('.mp4','.json'))\n",
    "    if not isExist:    \n",
    "        print(\"Processing \" + filename)\n",
    "        video = load_video(filename)\n",
    "        frames += run_cotracker(filename, video, df_pe_alphapose, df_pe_ViTPose)\n",
    "\n",
    "    \n",
    "customutils.writeJson(frames,'videos/results/cotracker3/person_keypoints_running.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py390",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
