{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "\n",
    "from base64 import b64encode\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from IPython.display import HTML\n",
    "from cotracker.predictor import CoTrackerPredictor, CoTrackerOnlinePredictor\n",
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "# model = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker2_online\").to(\"cuda\")\n",
    "DEFAULT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoTrackerPredictor(checkpoint=os.path.join('./co-tracker/checkpoints/cotracker2.pth'))#.to(DEFAULT_DEVICE)\n",
    "\n",
    "NUM_KEYPOINTS = 26\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "LABELS = customutils.get_classes('configs/halpe26_classes.txt')\n",
    "LABELS_COCO = customutils.get_classes('configs/coco_classes.txt')\n",
    "LABELS_FOOT = ['LBigToe','RBigToe','LSmallToe','RSmallToe','LHeel','RHeel']\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "df_running_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_step(window_frames, is_first_step, queries = None):\n",
    "        video_chunk = (\n",
    "            torch.tensor(np.stack(window_frames[-model.step * 2 :]), device=DEFAULT_DEVICE)\n",
    "            .float()\n",
    "            .permute(0, 3, 1, 2)[None]\n",
    "        )  # (1, T, 3, H, W)\n",
    "        if queries is None or len(queries) == 0:\n",
    "            return model(\n",
    "            video_chunk,\n",
    "            is_first_step=is_first_step\n",
    "            )\n",
    "        else:\n",
    "            if torch.cuda.is_available():\n",
    "                queries = queries.cuda()\n",
    "                \n",
    "            return model(\n",
    "                video_chunk,\n",
    "                is_first_step=is_first_step,\n",
    "                queries=queries[None]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_video(input_path, output_path, x_start, y_start, width, height):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    # Get the original video dimensions\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can change the codec based on your requirements\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
    "\n",
    "    # Iterate through the frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if we reach the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Crop the frame to the specified dimensions\n",
    "        cropped_frame = frame[y_start:y_start + height, x_start:x_start + width]\n",
    "\n",
    "        # Resize the cropped frame to the new dimensions\n",
    "        resized_frame = cv2.resize(cropped_frame, (width, height))\n",
    "\n",
    "        # Write the resized frame to the output video file\n",
    "        out.write(resized_frame)\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Close any open windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import peakutils\n",
    "from KeyFrameDetector.utils import convert_frame_to_grayscale\n",
    "\n",
    "def cotracker_model(video, filename, person_count, queries, queries_dict, visualize = True):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    cap = cv2.VideoCapture('./data/videos/' + filename)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    lstfrm = []\n",
    "    lstdiffMag = []\n",
    "    timeSpans = []\n",
    "    images = []\n",
    "    full_color = []\n",
    "    lastFrame = None\n",
    "\n",
    "    for i in range(length):\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        grayframe, blur_gray = convert_frame_to_grayscale(frame)\n",
    "\n",
    "        frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "        lstfrm.append(frame_number)\n",
    "        images.append(grayframe)\n",
    "        full_color.append(frame)\n",
    "        if frame_number == 0:\n",
    "            lastFrame = blur_gray\n",
    "\n",
    "        diff = cv2.subtract(blur_gray, lastFrame)\n",
    "        diffMag = cv2.countNonZero(diff)\n",
    "        lstdiffMag.append(diffMag)\n",
    "        lastFrame = blur_gray\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    \n",
    "    y = np.array(lstdiffMag)\n",
    "    base = peakutils.baseline(y, 2)\n",
    "    indices = peakutils.indexes(y-base, float(0.3), min_dist=1)\n",
    "\n",
    "    # reset to first frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    queries = queries_dict[0]\n",
    "    for i in indices:\n",
    "        if i < len(queries_dict):\n",
    "            queries = torch.vstack((queries, queries_dict[i]))\n",
    "            \n",
    "    if torch.cuda.is_available():\n",
    "        queries = queries.cuda()\n",
    "        \n",
    "    pred_tracks, pred_visibility = model(\n",
    "                video,\n",
    "                queries=queries[None]\n",
    "            )\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    new_pred_tracks = None    \n",
    "    new_pred_visibility = None    \n",
    "    keyframe_i = 0\n",
    "    total_kpts_count=NUM_KEYPOINTS*person_count\n",
    "\n",
    "    indices = np.insert(indices, 0, 0)\n",
    "    for i in range(len(pred_tracks[0])):\n",
    "        frame = pred_tracks[0][i]\n",
    "        frame_vis = pred_visibility[0][i]\n",
    "\n",
    "        j = keyframe_i*total_kpts_count\n",
    "        k = (keyframe_i+1)*total_kpts_count\n",
    "        if k >= frame.shape[0]:\n",
    "            k = frame.shape[0]-1\n",
    "        new_frame = frame[j:k]\n",
    "        new_frame_vis = frame_vis[j:k]\n",
    "        \n",
    "        if new_frame.shape[0] < total_kpts_count:\n",
    "            new_pred_tracks = torch.cat((new_pred_tracks, frame.unsqueeze(0)),dim=0)\n",
    "            new_pred_visibility = torch.cat((new_pred_visibility, frame_vis.unsqueeze(0)),dim=0)\n",
    "            continue\n",
    "\n",
    "        if new_pred_tracks is None:\n",
    "            new_pred_tracks = new_frame\n",
    "            new_pred_visibility = new_frame_vis\n",
    "        elif len(new_pred_tracks.shape) == 2:\n",
    "            new_pred_tracks = torch.stack((new_pred_tracks, new_frame),dim=0)\n",
    "            new_pred_visibility = torch.stack((new_pred_visibility, new_frame_vis),dim=0)\n",
    "        else:\n",
    "            new_pred_tracks = torch.cat((new_pred_tracks, new_frame.unsqueeze(0)),dim=0)\n",
    "            new_pred_visibility = torch.cat((new_pred_visibility, new_frame_vis.unsqueeze(0)),dim=0)\n",
    "            \n",
    "        if keyframe_i+1 < len(indices) and indices[keyframe_i+1] <= i+1:\n",
    "            keyframe_i += 1\n",
    "            \n",
    "    if visualize:\n",
    "        vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "        vis.visualize(video, torch.unsqueeze(new_pred_tracks, 0), torch.unsqueeze(new_pred_visibility, 0), filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "    \n",
    "    return pred_tracks, pred_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cotracker_model_2(filename, queries, queries_dict):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    for i, frame in enumerate(\n",
    "        iio.imiter(\n",
    "            './data/videos/' + filename,\n",
    "            plugin=\"FFMPEG\",\n",
    "            fps = 30\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        if i % model.step == 0 and i != 0:\n",
    "            pred_tracks, pred_visibility = _process_step(\n",
    "                window_frames,\n",
    "                is_first_step,\n",
    "                queries=queries #queries_dict[i]\n",
    "            )\n",
    "            is_first_step = False\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    # Processing the final video frames in case video length is not a multiple of model.step\n",
    "    pred_tracks, pred_visibility = _process_step(\n",
    "        window_frames[-(i % model.step) - model.step - 1 :],\n",
    "        is_first_step\n",
    "    )\n",
    "\n",
    "    seq_name = filename\n",
    "    video = torch.tensor(np.stack(window_frames), device=DEFAULT_DEVICE).permute(0, 3, 1, 2)[None]\n",
    "    vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "    vis.visualize(video, pred_tracks, pred_visibility, filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "\n",
    "    return pred_tracks, pred_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "filenames = df_running_annotations['file_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CoTrackerPredictor and create an instance of it. We'll use this object to estimate tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_video(filename):\n",
    "    video = read_video_from_path('./data/videos/' + filename)\n",
    "    video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        video = video.cuda()\n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking manually selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, json, re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    frames = defaultdict(list)\n",
    "    isExist = os.path.exists(file_path)\n",
    "    person_count = 0\n",
    "    if isExist:\n",
    "      annotations = []\n",
    "      with open(file_path, 'r') as f:\n",
    "          annotations = json.load(f)\n",
    "          \n",
    "          keypoints = defaultdict(list)\n",
    "          person_count = len(annotations['annotations'])\n",
    "          for person in annotations['annotations']:\n",
    "            framecount = len(person['frames'])\n",
    "            for frame_index in range(0, framecount):\n",
    "              points = []\n",
    "              if frame_index < framecount:\n",
    "                frame = person['frames'][str(frame_index)]\n",
    "                for node in frame['skeleton']['nodes']:\n",
    "                    points.append({'id': node['name'], 'x' : node['x'], 'y': node['y']})\n",
    "\n",
    "              if len(points) > 0:\n",
    "                keypoints[frame_index].append({'person': {'points': points}})\n",
    "\n",
    "          for frame in range(0, len(keypoints)):\n",
    "            frames[frame] = keypoints[frame]\n",
    "\n",
    "    return frames, person_count\n",
    "\n",
    "\n",
    "def get_queries_for_frame(frame_number, annotations):\n",
    "  i = 0\n",
    "\n",
    "  for person in annotations[frame_number]:\n",
    "    for point in person['person']['points']:\n",
    "      new_tensor = torch.tensor([float(frame_number), point['x'], point['y']])\n",
    "      if i == 0:\n",
    "        queries_for_frames = new_tensor\n",
    "      else:\n",
    "        queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "      i += 1\n",
    "\n",
    "  return queries_for_frames\n",
    "\n",
    "\n",
    "def get_queries_for_frames(start_frame, end_frame, annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(annotations)\n",
    "\n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    for person in annotations[frame]:\n",
    "      for point in person['person']['points']:\n",
    "        if 'occluded' in point and point['occluded'] == 'false':\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), point['x'], point['y']])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict\n",
    "\n",
    "def get_queries_from_pe(start_frame, end_frame, annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(annotations)\n",
    "\n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    for person in annotations[frame]:\n",
    "      for point in person['person']['points']:\n",
    "        if 'occluded' in point and point['occluded'] == 'false':\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), point['x'], point['y']])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(filename):\n",
    "    mask_file = df_running_annotations['file_name'] == filename\n",
    "    mask_frame = df_running_annotations['frame'] == 0\n",
    "    image_id = df_running_annotations.loc[mask_file & mask_frame]['image_id'].iloc[0]\n",
    "    return image_id\n",
    "    \n",
    "np_pred = 0\n",
    "def run_cotracker(filename, video):\n",
    "    annot_json = 'videos/annotations/' + filename.replace('.mp4', '') + '.json'\n",
    "    annotations, person_count = load_annotations(annot_json)\n",
    "\n",
    "    queries, queries_dict = get_queries_for_frames(0, -1, annotations, 1)\n",
    "    #queries = get_queries_for_frames(0, 1, annotations)\n",
    "        \n",
    "    pred_tracks, pred_visibility = cotracker_model(video, filename, person_count, queries, queries_dict)\n",
    "\n",
    "    num_frames = pred_tracks.cpu().shape[1]\n",
    "    np_pred = pred_tracks.cpu().numpy()\n",
    "    df = pd.DataFrame(index=range(num_frames), columns=[str(x) for x in range(num_frames)])\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        frame = {}\n",
    "        frame['image_id'] = int(get_image_id(filename)) + i\n",
    "        frame['category_id'] = 1\n",
    "\n",
    "        keypoints = []\n",
    "        for j in range(len(np_pred[0][i])):\n",
    "            if j % NUM_KEYPOINTS == NUM_KEYPOINTS - 1:\n",
    "                # save current set of keypoints and start new frame/pose\n",
    "                frame['keypoints'] = keypoints\n",
    "                frames.append(frame)\n",
    "                keypoints = []\n",
    "            keypoints.append(np_pred[0][i][j][0].astype(float))\n",
    "            keypoints.append(np_pred[0][i][j][1].astype(float))\n",
    "            keypoints.append(2)\n",
    "\n",
    "        frame['keypoints'] = keypoints\n",
    "        frames.append(frame)\n",
    "\n",
    "    customutils.writeJson(frames,'videos/results/cotracker/' + filename.replace('.mp4', '') + '.json')\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1920, 1080) to (1920, 1088) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ./videos/results/cotracker/World_Athletics_Women_10000m_Oregon_2022_5.mp4\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for filename in filenames:\n",
    "    if filename != \"World_Athletics_Women_10000m_Oregon_2022_5.mp4\":\n",
    "        continue\n",
    "    video = load_video(filename)\n",
    "    frames += run_cotracker(filename, video)\n",
    "    break\n",
    "\n",
    "    \n",
    "# customutils.writeJson(frames,'videos/results/cotracker/person_keypoints_running.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py390",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
