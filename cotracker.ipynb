{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "\n",
    "from base64 import b64encode\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from IPython.display import HTML\n",
    "from cotracker.predictor import CoTrackerPredictor, CoTrackerOnlinePredictor\n",
    "\n",
    "# model = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker2_online\").to(\"cuda\")\n",
    "DEFAULT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoTrackerOnlinePredictor(checkpoint=os.path.join('./co-tracker/checkpoints/cotracker2.pth')).to(DEFAULT_DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_step(window_frames, is_first_step, queries = None):\n",
    "        video_chunk = (\n",
    "            torch.tensor(np.stack(window_frames[-model.step * 2 :]), device=DEFAULT_DEVICE)\n",
    "            .float()\n",
    "            .permute(0, 3, 1, 2)[None]\n",
    "        )  # (1, T, 3, H, W)\n",
    "        if queries is None or len(queries) == 0:\n",
    "            return model(\n",
    "            video_chunk,\n",
    "            is_first_step=is_first_step\n",
    "            )\n",
    "        else:\n",
    "            if torch.cuda.is_available():\n",
    "                queries = queries.cuda()\n",
    "                \n",
    "            return model(\n",
    "                video_chunk,\n",
    "                is_first_step=is_first_step,\n",
    "                queries=queries[None]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_video(input_path, output_path, x_start, y_start, width, height):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    # Get the original video dimensions\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can change the codec based on your requirements\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
    "\n",
    "    # Iterate through the frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if we reach the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Crop the frame to the specified dimensions\n",
    "        cropped_frame = frame[y_start:y_start + height, x_start:x_start + width]\n",
    "\n",
    "        # Resize the cropped frame to the new dimensions\n",
    "        resized_frame = cv2.resize(cropped_frame, (width, height))\n",
    "\n",
    "        # Write the resized frame to the output video file\n",
    "        out.write(resized_frame)\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Close any open windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KeyFrameDetector.key_frame_detector import keyframeDetection\n",
    "\n",
    "source = './data/videos/World_Athletics_Women_Marathon_Oregon_2022_2.mp4'\n",
    "dest = './data/videos/World_Athletics_Women_Marathon_Oregon_2022_2_keyframes.mp4'\n",
    "keyframeDetection(source, dest, float(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import peakutils\n",
    "from KeyFrameDetector.utils import convert_frame_to_grayscale, prepare_dirs, plot_metrics\n",
    "\n",
    "def keyframeDetection(source, dest, Thres, plotMetrics=False, verbose=False):\n",
    "    \n",
    "    keyframePath = dest+'/keyFrames'\n",
    "    imageGridsPath = dest+'/imageGrids'\n",
    "    csvPath = dest+'/csvFile'\n",
    "    path2file = csvPath + '/output.csv'\n",
    "    prepare_dirs(keyframePath, imageGridsPath, csvPath)\n",
    "\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "  \n",
    "    if (cap.isOpened()== False):\n",
    "        print(\"Error opening video file\")\n",
    "\n",
    "    lstfrm = []\n",
    "    lstdiffMag = []\n",
    "    timeSpans = []\n",
    "    images = []\n",
    "    full_color = []\n",
    "    lastFrame = None\n",
    "    Start_time = time.process_time()\n",
    "    \n",
    "    # Read until video is completed\n",
    "    for i in range(length):\n",
    "        ret, frame = cap.read()\n",
    "        grayframe, blur_gray = convert_frame_to_grayscale(frame)\n",
    "\n",
    "        frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "        lstfrm.append(frame_number)\n",
    "        images.append(grayframe)\n",
    "        full_color.append(frame)\n",
    "        if frame_number == 0:\n",
    "            lastFrame = blur_gray\n",
    "\n",
    "        diff = cv2.subtract(blur_gray, lastFrame)\n",
    "        diffMag = cv2.countNonZero(diff)\n",
    "        lstdiffMag.append(diffMag)\n",
    "        stop_time = time.process_time()\n",
    "        time_Span = stop_time-Start_time\n",
    "        timeSpans.append(time_Span)\n",
    "        lastFrame = blur_gray\n",
    "\n",
    "    cap.release()\n",
    "    y = np.array(lstdiffMag)\n",
    "    base = peakutils.baseline(y, 2)\n",
    "    indices = peakutils.indexes(y-base, Thres, min_dist=1)\n",
    "    \n",
    "    ##plot to monitor the selected keyframe\n",
    "    if (plotMetrics):\n",
    "        plot_metrics(indices, lstfrm, lstdiffMag)\n",
    "\n",
    "    cnt = 1\n",
    "    for x in indices:\n",
    "        cv2.imwrite(os.path.join(keyframePath , 'keyframe'+ str(cnt) +'.jpg'), full_color[x])\n",
    "        cnt +=1\n",
    "        log_message = 'keyframe ' + str(cnt) + ' happened at ' + str(timeSpans[x]) + ' sec.'\n",
    "        if(verbose):\n",
    "            print(log_message)\n",
    "        with open(path2file, 'w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerows(log_message)\n",
    "            csvFile.close()\n",
    "\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920 > 1920\n",
      "1080 > 1072\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import imageio as iio\n",
    "\n",
    "video_path = './data/videos/World_Athletics_Women_Marathon_Oregon_2022_2.mp4'\n",
    "\n",
    "reader = iio.get_reader(video_path, 'ffmpeg')\n",
    "meta = reader.get_meta_data()\n",
    "width, height = meta['size']\n",
    "\n",
    "# Ensure dimensions are divisible by the macro block size\n",
    "new_width = (width // 16) * 16\n",
    "new_height = (height // 16) * 16\n",
    "\n",
    "print(str(width) + ' > ' + str(new_width))\n",
    "print(str(height) + ' > ' + str(new_height))\n",
    "\n",
    "output_video_path = './data/videos/World_Athletics_Women_Marathon_Oregon_2022_2_cropped.mp4'\n",
    "crop_video(video_path, output_video_path, x_start=0, y_start=0, width=new_width, height=new_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio as iio\n",
    "\n",
    "video_path = './data/videos/World_Athletics_Women_Marathon_Oregon_2022_2_cropped.mp4'\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Check if the video file is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Get the frames per second (fps) of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "window_frames = []\n",
    "window_frames2 = []\n",
    "for i, frame in enumerate(\n",
    "    iio.imiter(\n",
    "        video_path,\n",
    "        plugin=\"FFMPEG\",\n",
    "        fps = 30\n",
    "    )\n",
    "):\n",
    "    window_frames.append(frame)\n",
    "    \n",
    "# Read until video is completed\n",
    "for i in range(length):\n",
    "    ret, frame = cap.read()\n",
    "    window_frames2.append(frame)\n",
    "\n",
    "\n",
    "# Release the video capture object and close any open windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cotracker_model(filename, queries, queries_dict):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    cap = cv2.VideoCapture('./data/videos/' + filename)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for i in range(length):\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if i % model.step == 0:# and i != 0:\n",
    "            queries = queries_dict[i]\n",
    "            if is_first_step:\n",
    "                queries = torch.vstack((queries_dict[0], queries_dict[i]))\n",
    "            pred_tracks, pred_visibility = _process_step(\n",
    "                window_frames,\n",
    "                is_first_step,\n",
    "                queries=queries\n",
    "            )\n",
    "            is_first_step = False\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    # Processing the final video frames in case video length is not a multiple of model.step\n",
    "    pred_tracks, pred_visibility = _process_step(\n",
    "        window_frames[-(i % model.step) - model.step - 1 :],\n",
    "        is_first_step\n",
    "    )\n",
    "\n",
    "    seq_name = filename\n",
    "    video = torch.tensor(np.stack(window_frames), device=DEFAULT_DEVICE).permute(0, 3, 1, 2)[None]\n",
    "    vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "    vis.visualize(video, pred_tracks, pred_visibility, filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return pred_tracks, pred_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cotracker_model(filename, queries, queries_dict):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    for i, frame in enumerate(\n",
    "        iio.imiter(\n",
    "            './data/videos/' + filename,\n",
    "            plugin=\"FFMPEG\",\n",
    "            fps = 30\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        if i % model.step == 0 and i != 0:\n",
    "            pred_tracks, pred_visibility = _process_step(\n",
    "                window_frames,\n",
    "                is_first_step,\n",
    "                queries=queries #queries_dict[i]\n",
    "            )\n",
    "            is_first_step = False\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    # Processing the final video frames in case video length is not a multiple of model.step\n",
    "    pred_tracks, pred_visibility = _process_step(\n",
    "        window_frames[-(i % model.step) - model.step - 1 :],\n",
    "        is_first_step\n",
    "    )\n",
    "\n",
    "    seq_name = filename\n",
    "    video = torch.tensor(np.stack(window_frames), device=DEFAULT_DEVICE).permute(0, 3, 1, 2)[None]\n",
    "    vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "    vis.visualize(video, pred_tracks, pred_visibility, filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "\n",
    "    return pred_tracks, pred_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "filenames = df_running_annotations['file_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CoTrackerPredictor and create an instance of it. We'll use this object to estimate tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_video(filename):\n",
    "    video = read_video_from_path('./data/videos/' + filename)\n",
    "    video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        video = video.cuda()\n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking manually selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, json, re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    frames = defaultdict(list)\n",
    "    isExist = os.path.exists(file_path)\n",
    "    if isExist:\n",
    "      annotations = []\n",
    "      with open(file_path, 'r') as f:\n",
    "          annotations = json.load(f)\n",
    "          \n",
    "          keypoints = defaultdict(list)\n",
    "          for person in annotations['annotations']:\n",
    "            framecount = len(person['frames'])\n",
    "            for frame_index in range(0, framecount):\n",
    "              points = []\n",
    "              if frame_index < framecount:\n",
    "                frame = person['frames'][str(frame_index)]\n",
    "                for node in frame['skeleton']['nodes']:\n",
    "                    points.append({'id': node['name'], 'x' : node['x'], 'y': node['y']})\n",
    "\n",
    "              if len(points) > 0:\n",
    "                keypoints[frame_index].append({'person': {'points': points}})\n",
    "\n",
    "          for frame in range(0, len(keypoints)):\n",
    "            frames[frame] = keypoints[frame]\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "def get_queries_for_frame(frame_number, annotations):\n",
    "  i = 0\n",
    "\n",
    "  for person in annotations[frame_number]:\n",
    "    for point in person['person']['points']:\n",
    "      new_tensor = torch.tensor([float(frame_number), point['x'], point['y']])\n",
    "      if i == 0:\n",
    "        queries_for_frames = new_tensor\n",
    "      else:\n",
    "        queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "      i += 1\n",
    "\n",
    "  return queries_for_frames\n",
    "\n",
    "\n",
    "def get_queries_for_frames(start_frame, end_frame, annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    for person in annotations[frame]:\n",
    "      for point in person['person']['points']:\n",
    "        new_tensor = torch.tensor([float(frame), point['x'], point['y']])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_queries_for_frames(start_frame, end_frame, annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "  first_frame = None\n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    for person in annotations[frame]:\n",
    "      for point in person['person']['points']:\n",
    "        new_tensor = torch.tensor([float(frame), point['x'], point['y']])\n",
    "        if queries_for_frames is None:\n",
    "          # if first_frame is not None:\n",
    "          #   queries_for_frames = torch.vstack((first_frame, new_tensor))\n",
    "          # else:\n",
    "            first_frame = new_tensor\n",
    "            queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "    # if frame != 0:\n",
    "      # queries_for_frames = torch.vstack((frames_dict[0], queries_for_frames))\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(filename):\n",
    "    mask_file = df_running_annotations['file_name'] == filename\n",
    "    mask_frame = df_running_annotations['frame'] == 0\n",
    "    image_id = df_running_annotations.loc[mask_file & mask_frame]['image_id'].iloc[0]\n",
    "    return image_id\n",
    "    \n",
    "def run_cotracker(filename, video):\n",
    "    annot_json = 'videos/annotations/' + filename.replace('.mp4', '') + '.json'\n",
    "    annotations = load_annotations(annot_json)\n",
    "\n",
    "    queries, queries_dict = get_queries_for_frames(0, 36, annotations, 1)\n",
    "    #queries = get_queries_for_frames(0, 1, annotations)\n",
    "        \n",
    "    pred_tracks, pred_visibility = cotracker_model(filename, queries, queries_dict)\n",
    "\n",
    "    num_frames = pred_tracks.cpu().shape[1]\n",
    "    np_pred = pred_tracks.cpu().numpy()\n",
    "    df = pd.DataFrame(index=range(num_frames), columns=[str(x) for x in range(num_frames)])\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        frame = {}\n",
    "        frame['image_id'] = int(get_image_id(filename)) + i\n",
    "        frame['category_id'] = 1\n",
    "\n",
    "        keypoints = []\n",
    "        for j in range(len(np_pred[0][i])): #range(26):\n",
    "            if j % 26 == 25:\n",
    "                # save current set of keypoints and start new frame/pose\n",
    "                frame['keypoints'] = keypoints\n",
    "                frames.append(frame)\n",
    "                keypoints = []\n",
    "            keypoints.append(np_pred[0][i][j][0].astype(float))\n",
    "            keypoints.append(np_pred[0][i][j][1].astype(float))\n",
    "            keypoints.append(2)\n",
    "\n",
    "        frame['keypoints'] = keypoints\n",
    "        frames.append(frame)\n",
    "\n",
    "    customutils.writeJson(frames,'videos/results/cotracker/' + filename.replace('.mp4', '') + '.json')\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     video \u001b[38;5;241m=\u001b[39m load_video(filename)\n\u001b[0;32m----> 6\u001b[0m     frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mrun_cotracker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# customutils.writeJson(frames,'videos/results/cotracker/person_keypoints_running.json')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mrun_cotracker\u001b[0;34m(filename, video)\u001b[0m\n\u001b[1;32m     11\u001b[0m queries, queries_dict \u001b[38;5;241m=\u001b[39m get_queries_for_frames(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m36\u001b[39m, annotations, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#queries = get_queries_for_frames(0, 1, annotations)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m pred_tracks, pred_visibility \u001b[38;5;241m=\u001b[39m \u001b[43mcotracker_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m pred_tracks\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m np_pred \u001b[38;5;241m=\u001b[39m pred_tracks\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mcotracker_model\u001b[0;34m(filename, queries, queries_dict)\u001b[0m\n\u001b[1;32m     13\u001b[0m     queries \u001b[38;5;241m=\u001b[39m queries_dict[i]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# if is_first_step:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# queries = torch.vstack((queries_dict[0], queries_dict[i]))\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     pred_tracks, pred_visibility \u001b[38;5;241m=\u001b[39m \u001b[43m_process_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_first_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     is_first_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m window_frames\u001b[38;5;241m.\u001b[39mappend(frame)\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36m_process_step\u001b[0;34m(window_frames, is_first_step, queries)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_step\u001b[39m(window_frames, is_first_step, queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m         video_chunk \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, device\u001b[38;5;241m=\u001b[39mDEFAULT_DEVICE)\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m      6\u001b[0m         )  \u001b[38;5;66;03m# (1, T, 3, H, W)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m queries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(queries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m model(\n\u001b[1;32m      9\u001b[0m             video_chunk,\n\u001b[1;32m     10\u001b[0m             is_first_step\u001b[38;5;241m=\u001b[39mis_first_step\n\u001b[1;32m     11\u001b[0m             )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/numpy/core/shape_base.py:445\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for filename in filenames:\n",
    "    if filename != \"World_Athletics_Women_Marathon_Oregon_2022_2.mp4\":\n",
    "        continue\n",
    "    video = load_video(filename)\n",
    "    frames += run_cotracker(filename, video)\n",
    "    break\n",
    "\n",
    "    \n",
    "# customutils.writeJson(frames,'videos/results/cotracker/person_keypoints_running.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py390",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
