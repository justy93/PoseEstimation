{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "\n",
    "from base64 import b64encode\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from IPython.display import HTML\n",
    "from cotracker.predictor import CoTrackerPredictor, CoTrackerOnlinePredictor\n",
    "\n",
    "# model = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker2_online\").to(\"cuda\")\n",
    "DEFAULT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoTrackerOnlinePredictor(checkpoint=os.path.join('./co-tracker/checkpoints/cotracker2.pth')).to(DEFAULT_DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_step(window_frames, is_first_step, queries = None):\n",
    "        video_chunk = (\n",
    "            torch.tensor(np.stack(window_frames[-model.step * 2 :]), device=DEFAULT_DEVICE)\n",
    "            .float()\n",
    "            .permute(0, 3, 1, 2)[None]\n",
    "        )  # (1, T, 3, H, W)\n",
    "        if queries is None or len(queries) == 0:\n",
    "            return model(\n",
    "            video_chunk,\n",
    "            is_first_step=is_first_step\n",
    "            )\n",
    "        else:\n",
    "            if torch.cuda.is_available():\n",
    "                queries = queries.cuda()\n",
    "                \n",
    "            return model(\n",
    "                video_chunk,\n",
    "                is_first_step=is_first_step,\n",
    "                queries=queries[None]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_video(input_path, output_path, x_start, y_start, width, height):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    # Get the original video dimensions\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can change the codec based on your requirements\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
    "\n",
    "    # Iterate through the frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if we reach the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Crop the frame to the specified dimensions\n",
    "        cropped_frame = frame[y_start:y_start + height, x_start:x_start + width]\n",
    "\n",
    "        # Resize the cropped frame to the new dimensions\n",
    "        resized_frame = cv2.resize(cropped_frame, (width, height))\n",
    "\n",
    "        # Write the resized frame to the output video file\n",
    "        out.write(resized_frame)\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Close any open windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920 > 1920\n",
      "1080 > 1072\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import imageio as iio\n",
    "\n",
    "video_path = './data/videos/World_Athletics_Women_Marathon_Oregon_2022_2.mp4'\n",
    "\n",
    "reader = iio.get_reader(video_path, 'ffmpeg')\n",
    "meta = reader.get_meta_data()\n",
    "width, height = meta['size']\n",
    "\n",
    "# Ensure dimensions are divisible by the macro block size\n",
    "new_width = (width // 16) * 16\n",
    "new_height = (height // 16) * 16\n",
    "\n",
    "print(str(width) + ' > ' + str(new_width))\n",
    "print(str(height) + ' > ' + str(new_height))\n",
    "\n",
    "output_video_path = './data/videos/World_Athletics_Women_Marathon_Oregon_2022_2_cropped.mp4'\n",
    "crop_video(video_path, output_video_path, x_start=0, y_start=0, width=new_width, height=new_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - imshow() missing required argument 'winname' (pos 1)\n>  - imshow() missing required argument 'winname' (pos 1)\n>  - imshow() missing required argument 'winname' (pos 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Break the loop if the user presses 'q'\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m/\u001b[39m fps)) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - imshow() missing required argument 'winname' (pos 1)\n>  - imshow() missing required argument 'winname' (pos 1)\n>  - imshow() missing required argument 'winname' (pos 1)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = './data/videos/World_Athletics_Women_Marathon_Oregon_2022_2.mp4'\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video file is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Get the frames per second (fps) of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Iterate through the frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if we reach the end of the video\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close any open windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cotracker_model(filename, queries, queries_dict):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    for i, frame in enumerate(\n",
    "        iio.imiter(\n",
    "            './data/videos/' + filename,\n",
    "            plugin=\"FFMPEG\",\n",
    "            fps = 30\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        if i % model.step == 0 and i != 0:\n",
    "            pred_tracks, pred_visibility = _process_step(\n",
    "                window_frames,\n",
    "                is_first_step,\n",
    "                queries=queries #queries_dict[i]\n",
    "            )\n",
    "            is_first_step = False\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    # Processing the final video frames in case video length is not a multiple of model.step\n",
    "    pred_tracks, pred_visibility = _process_step(\n",
    "        window_frames[-(i % model.step) - model.step - 1 :],\n",
    "        is_first_step\n",
    "    )\n",
    "\n",
    "    seq_name = filename\n",
    "    video = torch.tensor(np.stack(window_frames), device=DEFAULT_DEVICE).permute(0, 3, 1, 2)[None]\n",
    "    vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "    vis.visualize(video, pred_tracks, pred_visibility, filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "\n",
    "    return pred_tracks, pred_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "filenames = df_running_annotations['file_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CoTrackerPredictor and create an instance of it. We'll use this object to estimate tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_video(filename):\n",
    "    video = read_video_from_path('./data/videos/' + filename)\n",
    "    video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        video = video.cuda()\n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking manually selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, json, re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    frames = defaultdict(list)\n",
    "    isExist = os.path.exists(file_path)\n",
    "    if isExist:\n",
    "      annotations = []\n",
    "      with open(file_path, 'r') as f:\n",
    "          annotations = json.load(f)\n",
    "          \n",
    "          keypoints = defaultdict(list)\n",
    "          for person in annotations['annotations']:\n",
    "            framecount = len(person['frames'])\n",
    "            for frame_index in range(0, framecount):\n",
    "              points = []\n",
    "              if frame_index < framecount:\n",
    "                frame = person['frames'][str(frame_index)]\n",
    "                for node in frame['skeleton']['nodes']:\n",
    "                    points.append({'id': node['name'], 'x' : node['x'], 'y': node['y']})\n",
    "\n",
    "              if len(points) > 0:\n",
    "                keypoints[frame_index].append({'person': {'points': points}})\n",
    "\n",
    "          for frame in range(0, len(keypoints)):\n",
    "            frames[frame] = keypoints[frame]\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "def get_queries_for_frame(frame_number, annotations):\n",
    "  i = 0\n",
    "\n",
    "  for person in annotations[frame_number]:\n",
    "    for point in person['person']['points']:\n",
    "      new_tensor = torch.tensor([float(frame_number), point['x'], point['y']])\n",
    "      if i == 0:\n",
    "        queries_for_frames = new_tensor\n",
    "      else:\n",
    "        queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "      i += 1\n",
    "\n",
    "  return queries_for_frames\n",
    "\n",
    "\n",
    "def get_queries_for_frames(start_frame, end_frame, annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    for person in annotations[frame]:\n",
    "      for point in person['person']['points']:\n",
    "        new_tensor = torch.tensor([float(frame), point['x'], point['y']])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(filename):\n",
    "    mask_file = df_running_annotations['file_name'] == filename\n",
    "    mask_frame = df_running_annotations['frame'] == 0\n",
    "    image_id = df_running_annotations.loc[mask_file & mask_frame]['image_id'].iloc[0]\n",
    "    return image_id\n",
    "    \n",
    "def run_cotracker(filename, video):\n",
    "    annot_json = 'videos/annotations/' + filename.replace('.mp4', '') + '.json'\n",
    "    annotations = load_annotations(annot_json)\n",
    "\n",
    "    queries, queries_dict = get_queries_for_frames(0, 36, annotations, 1)\n",
    "        \n",
    "    pred_tracks, pred_visibility = cotracker_model(filename.replace('.mp4', '_cropped.mp4'), queries, queries_dict)\n",
    "\n",
    "    num_frames = pred_tracks.cpu().shape[1]\n",
    "    np_pred = pred_tracks.cpu().numpy()\n",
    "    df = pd.DataFrame(index=range(num_frames), columns=[str(x) for x in range(num_frames)])\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        frame = {}\n",
    "        frame['image_id'] = int(get_image_id(filename)) + i\n",
    "        frame['category_id'] = 1\n",
    "\n",
    "        keypoints = []\n",
    "        for j in range(len(np_pred[0][i])): #range(26):\n",
    "            if j % 26 == 25:\n",
    "                # save current set of keypoints and start new frame/pose\n",
    "                frame['keypoints'] = keypoints\n",
    "                frames.append(frame)\n",
    "                keypoints = []\n",
    "            keypoints.append(np_pred[0][i][j][0].astype(float))\n",
    "            keypoints.append(np_pred[0][i][j][1].astype(float))\n",
    "            keypoints.append(2)\n",
    "\n",
    "        frame['keypoints'] = keypoints\n",
    "        frames.append(frame)\n",
    "\n",
    "    customutils.writeJson(frames,'videos/results/cotracker/' + filename.replace('.mp4', '') + '.json')\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ./videos/results/cotracker/World_Athletics_Women_Marathon_Oregon_2022_2_cropped.mp4\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for filename in filenames:\n",
    "    if filename != \"World_Athletics_Women_Marathon_Oregon_2022_2.mp4\":\n",
    "        continue\n",
    "    video = load_video(filename)\n",
    "    frames += run_cotracker(filename, video)\n",
    "    break\n",
    "\n",
    "    \n",
    "# customutils.writeJson(frames,'videos/results/cotracker/person_keypoints_running.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py390",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
