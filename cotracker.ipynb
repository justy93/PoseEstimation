{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>pose_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>frame</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>1003</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>1004</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12198</th>\n",
       "      <td>46160</td>\n",
       "      <td>146160</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12199</th>\n",
       "      <td>46161</td>\n",
       "      <td>146161</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12200</th>\n",
       "      <td>46162</td>\n",
       "      <td>146162</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12201</th>\n",
       "      <td>46163</td>\n",
       "      <td>146163</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12202</th>\n",
       "      <td>46164</td>\n",
       "      <td>146164</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12203 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  pose_id                                         file_name  \\\n",
       "0          1000     1000               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "1          1001     1001               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "2          1002     1002               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "3          1003     1003               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "4          1004     1004               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "...         ...      ...                                               ...   \n",
       "12198     46160   146160  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12199     46161   146161  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12200     46162   146162  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12201     46163   146163  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12202     46164   146164  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "\n",
       "       frame  person  \n",
       "0          0       0  \n",
       "1          1       0  \n",
       "2          2       0  \n",
       "3          3       0  \n",
       "4          4       0  \n",
       "...      ...     ...  \n",
       "12198    160       1  \n",
       "12199    161       1  \n",
       "12200    162       1  \n",
       "12201    163       1  \n",
       "12202    164       1  \n",
       "\n",
       "[12203 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "\n",
    "from base64 import b64encode\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from IPython.display import HTML\n",
    "from cotracker.predictor import CoTrackerPredictor, CoTrackerOnlinePredictor\n",
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "# model = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker2_online\").to(\"cuda\")\n",
    "DEFAULT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoTrackerPredictor(checkpoint=os.path.join('./co-tracker/checkpoints/cotracker2.pth'))#.to(DEFAULT_DEVICE)\n",
    "\n",
    "NUM_KEYPOINTS = 26\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "df_running_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_step(window_frames, is_first_step, queries = None):\n",
    "        video_chunk = (\n",
    "            torch.tensor(np.stack(window_frames[-model.step * 2 :]), device=DEFAULT_DEVICE)\n",
    "            .float()\n",
    "            .permute(0, 3, 1, 2)[None]\n",
    "        )  # (1, T, 3, H, W)\n",
    "        if queries is None or len(queries) == 0:\n",
    "            return model(\n",
    "            video_chunk,\n",
    "            is_first_step=is_first_step\n",
    "            )\n",
    "        else:\n",
    "            if torch.cuda.is_available():\n",
    "                queries = queries.cuda()\n",
    "                \n",
    "            return model(\n",
    "                video_chunk,\n",
    "                is_first_step=is_first_step,\n",
    "                queries=queries[None]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_video(input_path, output_path, x_start, y_start, width, height):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    # Get the original video dimensions\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can change the codec based on your requirements\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
    "\n",
    "    # Iterate through the frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if we reach the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Crop the frame to the specified dimensions\n",
    "        cropped_frame = frame[y_start:y_start + height, x_start:x_start + width]\n",
    "\n",
    "        # Resize the cropped frame to the new dimensions\n",
    "        resized_frame = cv2.resize(cropped_frame, (width, height))\n",
    "\n",
    "        # Write the resized frame to the output video file\n",
    "        out.write(resized_frame)\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Close any open windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import peakutils\n",
    "from KeyFrameDetector.utils import convert_frame_to_grayscale\n",
    "\n",
    "def cotracker_model(video, filename, model_name, person_count, queries, queries_dict, visualize = True):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    cap = cv2.VideoCapture('./data/videos/' + filename)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    lstfrm = []\n",
    "    lstdiffMag = []\n",
    "    timeSpans = []\n",
    "    images = []\n",
    "    full_color = []\n",
    "    lastFrame = None\n",
    "\n",
    "    for i in range(length):\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        grayframe, blur_gray = convert_frame_to_grayscale(frame)\n",
    "\n",
    "        frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "        lstfrm.append(frame_number)\n",
    "        images.append(grayframe)\n",
    "        full_color.append(frame)\n",
    "        if frame_number == 0:\n",
    "            lastFrame = blur_gray\n",
    "\n",
    "        diff = cv2.subtract(blur_gray, lastFrame)\n",
    "        diffMag = cv2.countNonZero(diff)\n",
    "        lstdiffMag.append(diffMag)\n",
    "        lastFrame = blur_gray\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    \n",
    "    y = np.array(lstdiffMag)\n",
    "    base = peakutils.baseline(y, 2)\n",
    "    indices = peakutils.indexes(y-base, float(0.3), min_dist=1)\n",
    "\n",
    "    # reset to first frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    queries = queries_dict[0]\n",
    "    idx = []\n",
    "    for i, ind in enumerate(indices):\n",
    "        if queries_dict[ind] is None:\n",
    "            idx.append(i)\n",
    "        elif ind < len(queries_dict):\n",
    "            queries = torch.vstack((queries, queries_dict[ind]))\n",
    "    indices = np.delete(indices, idx, axis=0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        queries = queries.cuda()\n",
    "        \n",
    "    pred_tracks, pred_visibility = model(\n",
    "                video,\n",
    "                queries=queries[None]\n",
    "            )\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    new_pred_tracks = None    \n",
    "    new_pred_visibility = None    \n",
    "    keyframe_i = 0\n",
    "    total_kpts_count=NUM_KEYPOINTS*person_count\n",
    "\n",
    "    indices = np.insert(indices, 0, 0)\n",
    "    for i in range(len(pred_tracks[0])):\n",
    "        frame = pred_tracks[0][i]\n",
    "        frame_vis = pred_visibility[0][i]\n",
    "\n",
    "        j = keyframe_i*total_kpts_count\n",
    "        k = (keyframe_i+1)*total_kpts_count\n",
    "        if k > frame.shape[0]:\n",
    "            k = frame.shape[0]-1\n",
    "        new_frame = frame[j:k]\n",
    "        new_frame_vis = frame_vis[j:k]\n",
    "        \n",
    "        if new_frame.shape[0] < total_kpts_count:\n",
    "            new_pred_tracks = torch.cat((new_pred_tracks, frame.unsqueeze(0)),dim=0)\n",
    "            new_pred_visibility = torch.cat((new_pred_visibility, frame_vis.unsqueeze(0)),dim=0)\n",
    "            continue\n",
    "\n",
    "        if new_pred_tracks is None:\n",
    "            new_pred_tracks = new_frame\n",
    "            new_pred_visibility = new_frame_vis\n",
    "        elif len(new_pred_tracks.shape) == 2:\n",
    "            new_pred_tracks = torch.stack((new_pred_tracks, new_frame),dim=0)\n",
    "            new_pred_visibility = torch.stack((new_pred_visibility, new_frame_vis),dim=0)\n",
    "        else:\n",
    "            new_pred_tracks = torch.cat((new_pred_tracks, new_frame.unsqueeze(0)),dim=0)\n",
    "            new_pred_visibility = torch.cat((new_pred_visibility, new_frame_vis.unsqueeze(0)),dim=0)\n",
    "            \n",
    "        if keyframe_i+1 < len(indices) and indices[keyframe_i+1] <= i+1:\n",
    "            keyframe_i += 1\n",
    "            \n",
    "    if visualize:\n",
    "        vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "        #vis.visualize(video, pred_tracks, pred_visibility, filename = model_name + '_' + filename.replace('.mp4', ''), query_frame=0)\n",
    "        vis.visualize(video, torch.unsqueeze(new_pred_tracks, 0), torch.unsqueeze(new_pred_visibility, 0), filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "    \n",
    "    return pred_tracks, pred_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cotracker_model_2(filename, queries, queries_dict):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    for i, frame in enumerate(\n",
    "        iio.imiter(\n",
    "            './data/videos/' + filename,\n",
    "            plugin=\"FFMPEG\",\n",
    "            fps = 30\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        if i % model.step == 0 and i != 0:\n",
    "            pred_tracks, pred_visibility = _process_step(\n",
    "                window_frames,\n",
    "                is_first_step,\n",
    "                queries=queries #queries_dict[i]\n",
    "            )\n",
    "            is_first_step = False\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    # Processing the final video frames in case video length is not a multiple of model.step\n",
    "    pred_tracks, pred_visibility = _process_step(\n",
    "        window_frames[-(i % model.step) - model.step - 1 :],\n",
    "        is_first_step\n",
    "    )\n",
    "\n",
    "    seq_name = filename\n",
    "    video = torch.tensor(np.stack(window_frames), device=DEFAULT_DEVICE).permute(0, 3, 1, 2)[None]\n",
    "    vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "    vis.visualize(video, pred_tracks, pred_visibility, filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "\n",
    "    return pred_tracks, pred_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Athletics_Mixed_Tokyo_2020_20_1.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_39.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_5.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_64.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_8.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_14.mp4', 'Marathon_Men_Tokyo_2020_16.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_28.mp4', 'Marathon_Men_Tokyo_2020_47.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_52.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_12_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_19_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_21_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_23.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_28.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_29.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_2_1.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_33_1.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_1.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_23.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_25.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_28.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_38.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_46.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_49.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_5.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_18.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_20.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_21.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_26.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_3.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_9.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_10.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_32.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_35.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_43.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_46.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_5.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_24.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_26.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_28.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_7.mp4',\n",
       "       'World_Athletics_Women_Marathon_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Women_Marathon_Oregon_2022_8.mp4'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "filenames = df_running_annotations['file_name'].unique()\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CoTrackerPredictor and create an instance of it. We'll use this object to estimate tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_video(filename):\n",
    "    video = read_video_from_path('./data/videos/' + filename)\n",
    "    video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        video = video.cuda()\n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking manually selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, json, re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    frames = defaultdict(list)\n",
    "    isExist = os.path.exists(file_path)\n",
    "    person_count = 0\n",
    "    if isExist:\n",
    "      annotations = []\n",
    "      with open(file_path, 'r') as f:\n",
    "          annotations = json.load(f)\n",
    "          \n",
    "          keypoints = defaultdict(list)\n",
    "          person_count = len(annotations['annotations'])\n",
    "          for person in annotations['annotations']:\n",
    "            framecount = len(person['frames'])\n",
    "            for frame_index in range(0, framecount):\n",
    "              points = []\n",
    "              if frame_index < framecount:\n",
    "                frame = person['frames'][str(frame_index)]\n",
    "                for node in frame['skeleton']['nodes']:\n",
    "                    points.append({'id': node['name'], 'x' : node['x'], 'y': node['y']})\n",
    "\n",
    "              if len(points) > 0:\n",
    "                keypoints[frame_index].append({'person': {'points': points}})\n",
    "\n",
    "          for frame in range(0, len(keypoints)):\n",
    "            frames[frame] = keypoints[frame]\n",
    "\n",
    "    return frames, person_count\n",
    "\n",
    "\n",
    "def get_queries_for_frame(frame_number, annotations):\n",
    "  i = 0\n",
    "\n",
    "  for person in annotations[frame_number]:\n",
    "    for point in person['person']['points']:\n",
    "      new_tensor = torch.tensor([float(frame_number), point['x'], point['y']])\n",
    "      if i == 0:\n",
    "        queries_for_frames = new_tensor\n",
    "      else:\n",
    "        queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "      i += 1\n",
    "\n",
    "  return queries_for_frames\n",
    "\n",
    "\n",
    "def get_queries_for_frames(start_frame, end_frame, annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(annotations)\n",
    "\n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    for person in annotations[frame]:\n",
    "      for point in person['person']['points']:\n",
    "        if 'occluded' in point and point['occluded'] == 'false':\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), point['x'], point['y']])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict\n",
    "\n",
    "\n",
    "def get_queries_from_pe(start_frame, end_frame, df, df_annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(df)\n",
    "\n",
    "  if df_annotations is not None and len(df_annotations) > 0:\n",
    "    # Find closest matching pose for first frame \n",
    "    poses = df[df['frame'] == 0]\n",
    "    gts = df_annotations[df_annotations['image_id'] == poses['image_id'].iloc[0]]\n",
    "    min_dist = np.inf\n",
    "    idx = -1\n",
    "    \n",
    "    for pose_id in gts.index:\n",
    "        gt = gts['keypoints'][pose_id]\n",
    "        kpts2, vi2 = customutils.edit_keypoints(gt)\n",
    "                                \n",
    "        area = customutils.compute_area_keypoints(gt)\n",
    "\n",
    "        # compute head size for distance normalization\n",
    "        head = customutils.get_keypoint(gt,\"head\")\n",
    "        neck = customutils.get_keypoint(gt,\"neck\")\n",
    "\n",
    "        headSize = 1\n",
    "        if (len(head) > 0 and len(neck) > 0):\n",
    "            headSize = customutils.get_head_size(head[0], head[1], neck[0], neck[1])\n",
    "            \n",
    "        for pose in poses.iloc:\n",
    "          if pose['score'] > .8:\n",
    "            dt = pose['keypoints']\n",
    "            kpts1, vi1 = customutils.edit_keypoints(dt)\n",
    "            d = np.linalg.norm(kpts1 - kpts2, ord=2, axis=1)\n",
    "            v = np.ones(len(d))\n",
    "\n",
    "            for part in range(len(d)):\n",
    "                if vi1[part] == 0 or vi2[part] == 0:\n",
    "                    d[part] = 0\n",
    "                    v[part] = 0\n",
    "\n",
    "            # normalize distance\n",
    "            dNorm = np.sum(d)/headSize\n",
    "\n",
    "            if dNorm < min_dist:\n",
    "              min_dist = dNorm\n",
    "              idx = pose['idx']\n",
    "\n",
    "  \n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    poses = df[df['frame'] == frame]\n",
    "    for pose in poses.iloc:\n",
    "\n",
    "      if idx == -1:\n",
    "        idx = pose['idx']\n",
    "\n",
    "      if idx != pose['idx']:\n",
    "        continue\n",
    "\n",
    "      kpts = pose['keypoints']\n",
    "      x, y, vi = customutils.get_x_y_v_keypoints(kpts)\n",
    "      for point in range(len(x)):\n",
    "        if vi[point] == 0:\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), float(x[point]), float(y[point])])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "      break\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = customutils.load_keypoints_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(filename):\n",
    "    mask_file = df_running_annotations['file_name'] == filename\n",
    "    mask_frame = df_running_annotations['frame'] == 0\n",
    "    image_id = df_running_annotations.loc[mask_file & mask_frame]['image_id'].iloc[0]\n",
    "    return image_id\n",
    "    \n",
    "def run_cotracker(filename, video, model_name, df_pe):\n",
    "    frames = []\n",
    "    annot_json = 'videos/annotations/' + filename.replace('.mp4', '') + '.json'\n",
    "    annotations, person_count = load_annotations(annot_json)\n",
    "\n",
    "    df_video_imageids = df_running_annotations[df_running_annotations['file_name'] == filename]\n",
    "    \n",
    "    df = df_pe[df_pe['image_id'].isin(df_video_imageids['image_id'])]\n",
    "    df = df_pe.merge(df_video_imageids, how='left', left_on='image_id', right_on='image_id')\n",
    "    \n",
    "    queries, queries_dict = get_queries_from_pe(0, -1, df, df_annotations, 1)\n",
    "    #queries = get_queries_for_frames(0, 1, annotations)\n",
    "    \n",
    "    pred_tracks, pred_visibility = cotracker_model(video, filename, model_name, 1, queries, queries_dict)\n",
    "\n",
    "    num_frames = pred_tracks.cpu().shape[1]\n",
    "    np_pred = pred_tracks.cpu().numpy()\n",
    "    df = pd.DataFrame(index=range(num_frames), columns=[str(x) for x in range(num_frames)])\n",
    "    for i in range(num_frames):\n",
    "        frame = {}\n",
    "        frame['image_id'] = int(get_image_id(filename)) + i\n",
    "        frame['category_id'] = 1\n",
    "\n",
    "        keypoints = []\n",
    "        for j in range(len(np_pred[0][i])):\n",
    "            if j % NUM_KEYPOINTS == NUM_KEYPOINTS - 1:\n",
    "                # save current set of keypoints and start new frame/pose\n",
    "                frame['keypoints'] = keypoints\n",
    "                frames.append(frame)\n",
    "                keypoints = []\n",
    "            keypoints.append(np_pred[0][i][j][0].astype(float))\n",
    "            keypoints.append(np_pred[0][i][j][1].astype(float))\n",
    "            keypoints.append(2)\n",
    "\n",
    "        frame['keypoints'] = keypoints\n",
    "        frames.append(frame)\n",
    "\n",
    "    customutils.writeJson(frames,'videos/results/cotracker/' + filename.replace('.mp4', '') + '.json')\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>keypoints</th>\n",
       "      <th>box</th>\n",
       "      <th>idx</th>\n",
       "      <th>score</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>[672.5870361328125, 397.4309997558594, 2, 692....</td>\n",
       "      <td>[455.3162841796875, 340.884765625, 402.0437011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>[1136.07421875, 266.266845703125, 2, 1155.6085...</td>\n",
       "      <td>[932.1016845703125, 190.0831756591797, 310.273...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.877560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>[682.8905029296875, 407.08642578125, 2, 702.61...</td>\n",
       "      <td>[456.44172066815923, 337.55727539062497, 405.4...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>[1134.0335693359375, 257.7659912109375, 2, 115...</td>\n",
       "      <td>[938.4007471361167, 194.7648895263672, 306.255...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.867543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>[691.739990234375, 412.57452392578125, 2, 713....</td>\n",
       "      <td>[443.5269343344591, 336.3762005969505, 431.860...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.859866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45401</th>\n",
       "      <td>46163</td>\n",
       "      <td>[704.8864135742188, 428.4450988769531, 2, 708....</td>\n",
       "      <td>[604.3679742712545, 384.44794226840435, 129.43...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.775203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45402</th>\n",
       "      <td>46163</td>\n",
       "      <td>[242.5023956298828, 475.6257019042969, 2, 245....</td>\n",
       "      <td>[219.46201027790286, 441.22925448208105, 108.8...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.555517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45403</th>\n",
       "      <td>46163</td>\n",
       "      <td>[1689.3052978515625, 480.3915100097656, 2, 168...</td>\n",
       "      <td>[1669.4820232953628, 467.83948263581397, 20.96...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.275808</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45404</th>\n",
       "      <td>46163</td>\n",
       "      <td>[1490.57373046875, 472.3777160644531, 2, 1493....</td>\n",
       "      <td>[1481.6382385788675, 450.34171338820823, 32.94...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.676191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45405</th>\n",
       "      <td>46163</td>\n",
       "      <td>[1005.9852294921875, 454.8758239746094, 2, 100...</td>\n",
       "      <td>[956.6008190511593, 409.1107207819066, 84.5862...</td>\n",
       "      <td>63</td>\n",
       "      <td>0.826677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45406 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                          keypoints  \\\n",
       "0          1000  [672.5870361328125, 397.4309997558594, 2, 692....   \n",
       "1          1000  [1136.07421875, 266.266845703125, 2, 1155.6085...   \n",
       "2          1001  [682.8905029296875, 407.08642578125, 2, 702.61...   \n",
       "3          1001  [1134.0335693359375, 257.7659912109375, 2, 115...   \n",
       "4          1002  [691.739990234375, 412.57452392578125, 2, 713....   \n",
       "...         ...                                                ...   \n",
       "45401     46163  [704.8864135742188, 428.4450988769531, 2, 708....   \n",
       "45402     46163  [242.5023956298828, 475.6257019042969, 2, 245....   \n",
       "45403     46163  [1689.3052978515625, 480.3915100097656, 2, 168...   \n",
       "45404     46163  [1490.57373046875, 472.3777160644531, 2, 1493....   \n",
       "45405     46163  [1005.9852294921875, 454.8758239746094, 2, 100...   \n",
       "\n",
       "                                                     box  idx     score  \\\n",
       "0      [455.3162841796875, 340.884765625, 402.0437011...    1  0.898572   \n",
       "1      [932.1016845703125, 190.0831756591797, 310.273...    2  0.877560   \n",
       "2      [456.44172066815923, 337.55727539062497, 405.4...    1  0.891666   \n",
       "3      [938.4007471361167, 194.7648895263672, 306.255...    2  0.867543   \n",
       "4      [443.5269343344591, 336.3762005969505, 431.860...    1  0.859866   \n",
       "...                                                  ...  ...       ...   \n",
       "45401  [604.3679742712545, 384.44794226840435, 129.43...   29  0.775203   \n",
       "45402  [219.46201027790286, 441.22925448208105, 108.8...   12  0.555517   \n",
       "45403  [1669.4820232953628, 467.83948263581397, 20.96...   37  0.275808   \n",
       "45404  [1481.6382385788675, 450.34171338820823, 32.94...   54  0.676191   \n",
       "45405  [956.6008190511593, 409.1107207819066, 84.5862...   63  0.826677   \n",
       "\n",
       "       category_id  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "45401            1  \n",
       "45402            1  \n",
       "45403            1  \n",
       "45404            1  \n",
       "45405            1  \n",
       "\n",
       "[45406 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_pe_alphapose = customutils.load_pe_dataframe('alphapose')\n",
    "df_pe_openpose = customutils.load_pe_dataframe('openpose')\n",
    "df_pe_ViTPose = customutils.load_pe_dataframe('ViTPose')\n",
    "\n",
    "df_pe_alphapose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Triathlon_Men_Tokyo_2020_19_1.mp4\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacty of 11.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 22.61 GiB is allocated by PyTorch, and 18.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename)\n\u001b[1;32m      9\u001b[0m         video \u001b[38;5;241m=\u001b[39m load_video(filename)\n\u001b[0;32m---> 10\u001b[0m         frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mrun_cotracker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malphapose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_pe_alphapose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# customutils.writeJson(frames,'videos/results/cotracker/person_keypoints_running.json')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mrun_cotracker\u001b[0;34m(filename, video, model_name, df_pe)\u001b[0m\n\u001b[1;32m     17\u001b[0m queries, queries_dict \u001b[38;5;241m=\u001b[39m get_queries_from_pe(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, df, df_annotations, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#queries = get_queries_for_frames(0, 1, annotations)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m pred_tracks, pred_visibility \u001b[38;5;241m=\u001b[39m \u001b[43mcotracker_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m pred_tracks\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     23\u001b[0m np_pred \u001b[38;5;241m=\u001b[39m pred_tracks\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m, in \u001b[0;36mcotracker_model\u001b[0;34m(video, filename, model_name, person_count, queries, queries_dict, visualize)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     58\u001b[0m     queries \u001b[38;5;241m=\u001b[39m queries\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 60\u001b[0m pred_tracks, pred_visibility \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     66\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/Justine/uni/co-tracker/cotracker/predictor.py:46\u001b[0m, in \u001b[0;36mCoTrackerPredictor.forward\u001b[0;34m(self, video, queries, segm_mask, grid_size, grid_query_frame, backward_tracking)\u001b[0m\n\u001b[1;32m     40\u001b[0m     tracks, visibilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dense_tracks(\n\u001b[1;32m     41\u001b[0m         video,\n\u001b[1;32m     42\u001b[0m         grid_query_frame\u001b[38;5;241m=\u001b[39mgrid_query_frame,\n\u001b[1;32m     43\u001b[0m         backward_tracking\u001b[38;5;241m=\u001b[39mbackward_tracking,\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     tracks, visibilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_sparse_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43msegm_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_support_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msegm_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrid_query_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_query_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackward_tracking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackward_tracking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tracks, visibilities\n",
      "File \u001b[0;32m/mnt/d/Justine/uni/co-tracker/cotracker/predictor.py:133\u001b[0m, in \u001b[0;36mCoTrackerPredictor._compute_sparse_tracks\u001b[0;34m(self, video, queries, segm_mask, grid_size, add_support_grid, grid_query_frame, backward_tracking)\u001b[0m\n\u001b[1;32m    130\u001b[0m     grid_pts \u001b[38;5;241m=\u001b[39m grid_pts\u001b[38;5;241m.\u001b[39mrepeat(B, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    131\u001b[0m     queries \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([queries, grid_pts], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m tracks, visibilities, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backward_tracking:\n\u001b[1;32m    136\u001b[0m     tracks, visibilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_backward_tracks(\n\u001b[1;32m    137\u001b[0m         video, queries, tracks, visibilities\n\u001b[1;32m    138\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/d/Justine/uni/co-tracker/cotracker/models/core/cotracker/cotracker.py:261\u001b[0m, in \u001b[0;36mCoTracker2.forward\u001b[0;34m(self, video, queries, iters, is_train, is_online)\u001b[0m\n\u001b[1;32m    256\u001b[0m video \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(video\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m1\u001b[39m, T, C \u001b[38;5;241m*\u001b[39m H \u001b[38;5;241m*\u001b[39m W), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, pad), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    257\u001b[0m     B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C, H, W\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Compute convolutional features for the video or for the current chunk in case of online mode\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m fmaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    262\u001b[0m     B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim, H \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, W \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride\n\u001b[1;32m    263\u001b[0m )\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# We compute track features\u001b[39;00m\n\u001b[1;32m    266\u001b[0m track_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_track_feat(\n\u001b[1;32m    267\u001b[0m     fmaps,\n\u001b[1;32m    268\u001b[0m     queried_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_ind \u001b[38;5;28;01mif\u001b[39;00m is_online \u001b[38;5;28;01melse\u001b[39;00m queried_frames,\n\u001b[1;32m    269\u001b[0m     queried_coords,\n\u001b[1;32m    270\u001b[0m )\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, S, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/Justine/uni/co-tracker/cotracker/models/core/cotracker/blocks.py:194\u001b[0m, in \u001b[0;36mBasicEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    191\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m    192\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(x)\n\u001b[0;32m--> 194\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(a)\n\u001b[1;32m    196\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(b)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/Justine/uni/co-tracker/cotracker/models/core/cotracker/blocks.py:129\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    128\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(y)))\n\u001b[0;32m--> 129\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:87\u001b[0m, in \u001b[0;36m_InstanceNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_no_batch_dim():\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_no_batch_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_instance_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:36\u001b[0m, in \u001b[0;36m_InstanceNorm._apply_instance_norm\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_instance_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/torch/nn/functional.py:2523\u001b[0m, in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_input_stats:\n\u001b[1;32m   2522\u001b[0m     _verify_spatial_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_input_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacty of 11.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 22.61 GiB is allocated by PyTorch, and 18.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "for filename in filenames:\n",
    "    if filename == \"Marathon_Men_Tokyo_2020_16.mp4\" or filename == \"Triathlon_Men_Tokyo_2020_19_1.mp4\":\n",
    "        continue\n",
    "    isExist = os.path.exists(\"./videos/results/cotracker/\" + filename)\n",
    "    if not isExist:    \n",
    "        print(\"Processing \" + filename)\n",
    "        video = load_video(filename)\n",
    "        frames += run_cotracker(filename, video, 'alphapose', df_pe_alphapose)\n",
    "\n",
    "    \n",
    "# customutils.writeJson(frames,'videos/results/cotracker/person_keypoints_running.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py390",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
