{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>pose_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>frame</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>1003</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>1004</td>\n",
       "      <td>Athletics_Mixed_Tokyo_2020_20_1.mp4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12198</th>\n",
       "      <td>46160</td>\n",
       "      <td>146160</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12199</th>\n",
       "      <td>46161</td>\n",
       "      <td>146161</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12200</th>\n",
       "      <td>46162</td>\n",
       "      <td>146162</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12201</th>\n",
       "      <td>46163</td>\n",
       "      <td>146163</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12202</th>\n",
       "      <td>46164</td>\n",
       "      <td>146164</td>\n",
       "      <td>World_Athletics_Women_Marathon_Oregon_2022_8.mp4</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12203 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  pose_id                                         file_name  \\\n",
       "0          1000     1000               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "1          1001     1001               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "2          1002     1002               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "3          1003     1003               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "4          1004     1004               Athletics_Mixed_Tokyo_2020_20_1.mp4   \n",
       "...         ...      ...                                               ...   \n",
       "12198     46160   146160  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12199     46161   146161  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12200     46162   146162  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12201     46163   146163  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "12202     46164   146164  World_Athletics_Women_Marathon_Oregon_2022_8.mp4   \n",
       "\n",
       "       frame  person  \n",
       "0          0       0  \n",
       "1          1       0  \n",
       "2          2       0  \n",
       "3          3       0  \n",
       "4          4       0  \n",
       "...      ...     ...  \n",
       "12198    160       1  \n",
       "12199    161       1  \n",
       "12200    162       1  \n",
       "12201    163       1  \n",
       "12202    164       1  \n",
       "\n",
       "[12203 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "\n",
    "from base64 import b64encode\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from IPython.display import HTML\n",
    "from cotracker.predictor import CoTrackerPredictor, CoTrackerOnlinePredictor\n",
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "# model = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker2_online\").to(\"cuda\")\n",
    "DEFAULT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoTrackerPredictor(checkpoint=os.path.join('./co-tracker/checkpoints/cotracker2.pth'))#.to(DEFAULT_DEVICE)\n",
    "\n",
    "NUM_KEYPOINTS = 26\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "df_running_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_step(window_frames, is_first_step, queries = None):\n",
    "        video_chunk = (\n",
    "            torch.tensor(np.stack(window_frames[-model.step * 2 :]), device=DEFAULT_DEVICE)\n",
    "            .float()\n",
    "            .permute(0, 3, 1, 2)[None]\n",
    "        )  # (1, T, 3, H, W)\n",
    "        if queries is None or len(queries) == 0:\n",
    "            return model(\n",
    "            video_chunk,\n",
    "            is_first_step=is_first_step\n",
    "            )\n",
    "        else:\n",
    "            if torch.cuda.is_available():\n",
    "                queries = queries.cuda()\n",
    "                \n",
    "            return model(\n",
    "                video_chunk,\n",
    "                is_first_step=is_first_step,\n",
    "                queries=queries[None]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_video(input_path, output_path, x_start, y_start, width, height):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    # Get the original video dimensions\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can change the codec based on your requirements\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
    "\n",
    "    # Iterate through the frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if we reach the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Crop the frame to the specified dimensions\n",
    "        cropped_frame = frame[y_start:y_start + height, x_start:x_start + width]\n",
    "\n",
    "        # Resize the cropped frame to the new dimensions\n",
    "        resized_frame = cv2.resize(cropped_frame, (width, height))\n",
    "\n",
    "        # Write the resized frame to the output video file\n",
    "        out.write(resized_frame)\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Close any open windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import peakutils\n",
    "from KeyFrameDetector.utils import convert_frame_to_grayscale\n",
    "\n",
    "def cotracker_model(video, filename, model_name, person_count, queries, queries_dict, visualize = True):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    cap = cv2.VideoCapture('./data/videos/' + filename)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    lstfrm = []\n",
    "    lstdiffMag = []\n",
    "    timeSpans = []\n",
    "    images = []\n",
    "    full_color = []\n",
    "    lastFrame = None\n",
    "\n",
    "    for i in range(length):\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        grayframe, blur_gray = convert_frame_to_grayscale(frame)\n",
    "\n",
    "        frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "        lstfrm.append(frame_number)\n",
    "        images.append(grayframe)\n",
    "        full_color.append(frame)\n",
    "        if frame_number == 0:\n",
    "            lastFrame = blur_gray\n",
    "\n",
    "        diff = cv2.subtract(blur_gray, lastFrame)\n",
    "        diffMag = cv2.countNonZero(diff)\n",
    "        lstdiffMag.append(diffMag)\n",
    "        lastFrame = blur_gray\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    \n",
    "    y = np.array(lstdiffMag)\n",
    "    base = peakutils.baseline(y, 2)\n",
    "    indices = peakutils.indexes(y-base, float(0.3), min_dist=1)\n",
    "\n",
    "    # reset to first frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    queries = queries_dict[0]\n",
    "    idx = []\n",
    "    for i, ind in enumerate(indices):\n",
    "        if queries_dict[ind] is None:\n",
    "            idx.append(i)\n",
    "        elif ind < len(queries_dict):\n",
    "            queries = torch.vstack((queries, queries_dict[ind]))\n",
    "    indices = np.delete(indices, idx, axis=0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        queries = queries.cuda()\n",
    "        \n",
    "    pred_tracks, pred_visibility = model(\n",
    "                video,\n",
    "                queries=queries[None]\n",
    "            )\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    new_pred_tracks = None    \n",
    "    new_pred_visibility = None    \n",
    "    keyframe_i = 0\n",
    "    total_kpts_count=NUM_KEYPOINTS*person_count\n",
    "\n",
    "    indices = np.insert(indices, 0, 0)\n",
    "    for i in range(len(pred_tracks[0])):\n",
    "        frame = pred_tracks[0][i]\n",
    "        frame_vis = pred_visibility[0][i]\n",
    "\n",
    "        j = keyframe_i*total_kpts_count\n",
    "        k = (keyframe_i+1)*total_kpts_count\n",
    "        if k > frame.shape[0]:\n",
    "            k = frame.shape[0]-1\n",
    "        new_frame = frame[j:k]\n",
    "        new_frame_vis = frame_vis[j:k]\n",
    "        \n",
    "        if new_frame.shape[0] < total_kpts_count:\n",
    "            new_pred_tracks = torch.cat((new_pred_tracks, frame.unsqueeze(0)),dim=0)\n",
    "            new_pred_visibility = torch.cat((new_pred_visibility, frame_vis.unsqueeze(0)),dim=0)\n",
    "            continue\n",
    "\n",
    "        if new_pred_tracks is None:\n",
    "            new_pred_tracks = new_frame\n",
    "            new_pred_visibility = new_frame_vis\n",
    "        elif len(new_pred_tracks.shape) == 2:\n",
    "            new_pred_tracks = torch.stack((new_pred_tracks, new_frame),dim=0)\n",
    "            new_pred_visibility = torch.stack((new_pred_visibility, new_frame_vis),dim=0)\n",
    "        else:\n",
    "            new_pred_tracks = torch.cat((new_pred_tracks, new_frame.unsqueeze(0)),dim=0)\n",
    "            new_pred_visibility = torch.cat((new_pred_visibility, new_frame_vis.unsqueeze(0)),dim=0)\n",
    "            \n",
    "        if keyframe_i+1 < len(indices) and indices[keyframe_i+1] <= i+1:\n",
    "            keyframe_i += 1\n",
    "            \n",
    "    if visualize:\n",
    "        vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "        #vis.visualize(video, pred_tracks, pred_visibility, filename = model_name + '_' + filename.replace('.mp4', ''), query_frame=0)\n",
    "        vis.visualize(video, torch.unsqueeze(new_pred_tracks, 0), torch.unsqueeze(new_pred_visibility, 0), filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "    \n",
    "    return torch.unsqueeze(new_pred_tracks, 0), torch.unsqueeze(new_pred_visibility, 0), indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cotracker_model_2(filename, queries, queries_dict):\n",
    "    # Iterating over video frames, processing one window at a time:\n",
    "    is_first_step = True\n",
    "    window_frames = []\n",
    "\n",
    "    for i, frame in enumerate(\n",
    "        iio.imiter(\n",
    "            './data/videos/' + filename,\n",
    "            plugin=\"FFMPEG\",\n",
    "            fps = 30\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        if i % model.step == 0 and i != 0:\n",
    "            pred_tracks, pred_visibility = _process_step(\n",
    "                window_frames,\n",
    "                is_first_step,\n",
    "                queries=queries #queries_dict[i]\n",
    "            )\n",
    "            is_first_step = False\n",
    "        window_frames.append(frame)\n",
    "\n",
    "    # Processing the final video frames in case video length is not a multiple of model.step\n",
    "    pred_tracks, pred_visibility = _process_step(\n",
    "        window_frames[-(i % model.step) - model.step - 1 :],\n",
    "        is_first_step\n",
    "    )\n",
    "\n",
    "    seq_name = filename\n",
    "    video = torch.tensor(np.stack(window_frames), device=DEFAULT_DEVICE).permute(0, 3, 1, 2)[None]\n",
    "    vis = Visualizer(save_dir=\"./videos/results/cotracker/\", linewidth=3)\n",
    "    vis.visualize(video, pred_tracks, pred_visibility, filename=filename.replace('.mp4', ''), query_frame=0)\n",
    "\n",
    "    return pred_tracks, pred_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Athletics_Mixed_Tokyo_2020_20_1.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_39.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_5.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_64.mp4',\n",
       "       'Athletics_Mixed_Tokyo_2020_8.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_14.mp4', 'Marathon_Men_Tokyo_2020_16.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_28.mp4', 'Marathon_Men_Tokyo_2020_47.mp4',\n",
       "       'Marathon_Men_Tokyo_2020_52.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_12_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_19_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_21_1.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_23.mp4',\n",
       "       'Triathlon_Men_Tokyo_2020_28.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_29.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_2_1.mp4',\n",
       "       'Triathlon_Women_Tokyo_2020_33_1.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_1.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_23.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_25.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_28.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_38.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_46.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_49.mp4',\n",
       "       'World_Athletics_Men_10000m_Oregon_2022_5.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_18.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_20.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_21.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_26.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_3.mp4',\n",
       "       'World_Athletics_Men_5000m_Oregon_2022_9.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_10.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_32.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_35.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_43.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_46.mp4',\n",
       "       'World_Athletics_Women_10000m_Oregon_2022_5.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_24.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_26.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_28.mp4',\n",
       "       'World_Athletics_Women_5000m_Oregon_2022_7.mp4',\n",
       "       'World_Athletics_Women_Marathon_Oregon_2022_2.mp4',\n",
       "       'World_Athletics_Women_Marathon_Oregon_2022_8.mp4'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PoseEstimation.customutils as customutils\n",
    "\n",
    "df_running_annotations = customutils.load_images_dataframe()\n",
    "filenames = df_running_annotations['file_name'].unique()\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CoTrackerPredictor and create an instance of it. We'll use this object to estimate tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_video(filename):\n",
    "    video = read_video_from_path('./data/videos/' + filename)\n",
    "    video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        video = video.cuda()\n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking manually selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, json, re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    frames = defaultdict(list)\n",
    "    isExist = os.path.exists(file_path)\n",
    "    person_count = 0\n",
    "    if isExist:\n",
    "      annotations = []\n",
    "      with open(file_path, 'r') as f:\n",
    "          annotations = json.load(f)\n",
    "          \n",
    "          keypoints = defaultdict(list)\n",
    "          person_count = len(annotations['annotations'])\n",
    "          for person in annotations['annotations']:\n",
    "            framecount = len(person['frames'])\n",
    "            for frame_index in range(0, framecount):\n",
    "              points = []\n",
    "              if frame_index < framecount:\n",
    "                frame = person['frames'][str(frame_index)]\n",
    "                for node in frame['skeleton']['nodes']:\n",
    "                    points.append({'id': node['name'], 'x' : node['x'], 'y': node['y']})\n",
    "\n",
    "              if len(points) > 0:\n",
    "                keypoints[frame_index].append({'person': {'points': points}})\n",
    "\n",
    "          for frame in range(0, len(keypoints)):\n",
    "            frames[frame] = keypoints[frame]\n",
    "\n",
    "    return frames, person_count\n",
    "\n",
    "\n",
    "def get_queries_for_frame(frame_number, annotations):\n",
    "  i = 0\n",
    "\n",
    "  for person in annotations[frame_number]:\n",
    "    for point in person['person']['points']:\n",
    "      new_tensor = torch.tensor([float(frame_number), point['x'], point['y']])\n",
    "      if i == 0:\n",
    "        queries_for_frames = new_tensor\n",
    "      else:\n",
    "        queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "      i += 1\n",
    "\n",
    "  return queries_for_frames\n",
    "\n",
    "\n",
    "def get_queries_for_frames(start_frame, end_frame, annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(annotations)\n",
    "\n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    for person in annotations[frame]:\n",
    "      for point in person['person']['points']:\n",
    "        if 'occluded' in point and point['occluded'] == 'false':\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), point['x'], point['y']])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict\n",
    "\n",
    "\n",
    "def get_queries_from_pe(start_frame, end_frame, df, df_annotations, step = 4):\n",
    "  frame = 0\n",
    "  frames_dict = defaultdict(list)\n",
    "  all_queries_for_frames = None\n",
    "\n",
    "  if end_frame == -1:\n",
    "    end_frame = len(df)\n",
    "\n",
    "  if df_annotations is not None and len(df_annotations) > 0:\n",
    "    # Find closest matching pose for first frame \n",
    "    poses = df[df['frame'] == 0]\n",
    "    gts = df_annotations[df_annotations['image_id'] == poses['image_id'].iloc[0]]\n",
    "    min_dist = np.inf\n",
    "    idx = -1\n",
    "    \n",
    "    for pose_id in gts.index:\n",
    "        gt = gts['keypoints'][pose_id]\n",
    "        kpts2, vi2 = customutils.edit_keypoints(gt)\n",
    "                                \n",
    "        area = customutils.compute_area_keypoints(gt)\n",
    "\n",
    "        # compute head size for distance normalization\n",
    "        head = customutils.get_keypoint(gt,\"head\")\n",
    "        neck = customutils.get_keypoint(gt,\"neck\")\n",
    "\n",
    "        headSize = 1\n",
    "        if (len(head) > 0 and len(neck) > 0):\n",
    "            headSize = customutils.get_head_size(head[0], head[1], neck[0], neck[1])\n",
    "            \n",
    "        for pose in poses.iloc:\n",
    "          if pose['score'] > .8:\n",
    "            dt = pose['keypoints']\n",
    "            kpts1, vi1 = customutils.edit_keypoints(dt)\n",
    "            d = np.linalg.norm(kpts1 - kpts2, ord=2, axis=1)\n",
    "            v = np.ones(len(d))\n",
    "\n",
    "            for part in range(len(d)):\n",
    "                if vi1[part] == 0 or vi2[part] == 0:\n",
    "                    d[part] = 0\n",
    "                    v[part] = 0\n",
    "\n",
    "            # normalize distance\n",
    "            dNorm = np.sum(d)/headSize\n",
    "\n",
    "            if dNorm < min_dist:\n",
    "              min_dist = dNorm\n",
    "              idx = pose['idx']\n",
    "\n",
    "  \n",
    "  for frame in range(start_frame, end_frame, step):\n",
    "    queries_for_frames = None\n",
    "    poses = df[df['frame'] == frame]\n",
    "    for pose in poses.iloc:\n",
    "\n",
    "      if idx == -1:\n",
    "        idx = pose['idx']\n",
    "\n",
    "      if idx != pose['idx']:\n",
    "        continue\n",
    "\n",
    "      kpts = pose['keypoints']\n",
    "      x, y, vi = customutils.get_x_y_v_keypoints(kpts)\n",
    "      for point in range(len(x)):\n",
    "        if vi[point] == 0:\n",
    "          continue\n",
    "        new_tensor = torch.tensor([float(frame), float(x[point]), float(y[point])])\n",
    "        if queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "        if all_queries_for_frames is None:\n",
    "          queries_for_frames = new_tensor\n",
    "          all_queries_for_frames = new_tensor\n",
    "        else:\n",
    "          queries_for_frames = torch.vstack((queries_for_frames, new_tensor))\n",
    "          all_queries_for_frames = torch.vstack((all_queries_for_frames, new_tensor))\n",
    "\n",
    "      break\n",
    "\n",
    "    frames_dict[frame] = queries_for_frames\n",
    "\n",
    "  return all_queries_for_frames, frames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = customutils.load_keypoints_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(filename):\n",
    "    mask_file = df_running_annotations['file_name'] == filename\n",
    "    mask_frame = df_running_annotations['frame'] == 0\n",
    "    image_id = df_running_annotations.loc[mask_file & mask_frame]['image_id'].iloc[0]\n",
    "    return image_id\n",
    "    \n",
    "def run_cotracker(filename, video, model_name, df_pe):\n",
    "    frames = []\n",
    "    annot_json = 'videos/annotations/' + filename.replace('.mp4', '') + '.json'\n",
    "    annotations, person_count = load_annotations(annot_json)\n",
    "\n",
    "    df_video_imageids = df_running_annotations[df_running_annotations['file_name'] == filename]\n",
    "    \n",
    "    df = df_pe[df_pe['image_id'].isin(df_video_imageids['image_id'])]\n",
    "    df = df_pe.merge(df_video_imageids, how='left', left_on='image_id', right_on='image_id')\n",
    "    \n",
    "    queries, queries_dict = get_queries_from_pe(0, -1, df, df_annotations, 1)\n",
    "    #queries = get_queries_for_frames(0, 1, annotations)\n",
    "    \n",
    "    pred_tracks, pred_visibility, indices = cotracker_model(video, filename, model_name, 1, queries, queries_dict)\n",
    "\n",
    "    num_frames = pred_tracks.cpu().shape[1]\n",
    "    np_pred = pred_tracks.cpu().numpy()\n",
    "    df = pd.DataFrame(index=range(num_frames), columns=[str(x) for x in range(num_frames)])\n",
    "    for i in range(num_frames):\n",
    "        frame = {}\n",
    "        frame['image_id'] = int(get_image_id(filename)) + i\n",
    "        frame['category_id'] = 1\n",
    "\n",
    "        keypoints = []\n",
    "        for j in range(len(np_pred[0][i])):\n",
    "            if j % NUM_KEYPOINTS == NUM_KEYPOINTS:\n",
    "                # save current set of keypoints and start new frame/pose\n",
    "                frame['keypoints'] = keypoints\n",
    "                frames.append(frame)\n",
    "                keypoints = []\n",
    "            keypoints.append(np_pred[0][i][j][0].astype(float))\n",
    "            keypoints.append(np_pred[0][i][j][1].astype(float))\n",
    "            keypoints.append(2)\n",
    "\n",
    "        frame['keypoints'] = keypoints\n",
    "        frames.append(frame)\n",
    "\n",
    "    customutils.writeJson({'frames': frames, 'keyframes': indices.tolist()},'videos/results/cotracker/' + filename.replace('.mp4', '') + '.json')\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>keypoints</th>\n",
       "      <th>box</th>\n",
       "      <th>idx</th>\n",
       "      <th>score</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>[672.5870361328125, 397.4309997558594, 2, 692....</td>\n",
       "      <td>[455.3162841796875, 340.884765625, 402.0437011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>[1136.07421875, 266.266845703125, 2, 1155.6085...</td>\n",
       "      <td>[932.1016845703125, 190.0831756591797, 310.273...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.877560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>[682.8905029296875, 407.08642578125, 2, 702.61...</td>\n",
       "      <td>[456.44172066815923, 337.55727539062497, 405.4...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>[1134.0335693359375, 257.7659912109375, 2, 115...</td>\n",
       "      <td>[938.4007471361167, 194.7648895263672, 306.255...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.867543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>[691.739990234375, 412.57452392578125, 2, 713....</td>\n",
       "      <td>[443.5269343344591, 336.3762005969505, 431.860...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.859866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45401</th>\n",
       "      <td>46163</td>\n",
       "      <td>[704.8864135742188, 428.4450988769531, 2, 708....</td>\n",
       "      <td>[604.3679742712545, 384.44794226840435, 129.43...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.775203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45402</th>\n",
       "      <td>46163</td>\n",
       "      <td>[242.5023956298828, 475.6257019042969, 2, 245....</td>\n",
       "      <td>[219.46201027790286, 441.22925448208105, 108.8...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.555517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45403</th>\n",
       "      <td>46163</td>\n",
       "      <td>[1689.3052978515625, 480.3915100097656, 2, 168...</td>\n",
       "      <td>[1669.4820232953628, 467.83948263581397, 20.96...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.275808</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45404</th>\n",
       "      <td>46163</td>\n",
       "      <td>[1490.57373046875, 472.3777160644531, 2, 1493....</td>\n",
       "      <td>[1481.6382385788675, 450.34171338820823, 32.94...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.676191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45405</th>\n",
       "      <td>46163</td>\n",
       "      <td>[1005.9852294921875, 454.8758239746094, 2, 100...</td>\n",
       "      <td>[956.6008190511593, 409.1107207819066, 84.5862...</td>\n",
       "      <td>63</td>\n",
       "      <td>0.826677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45406 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                          keypoints  \\\n",
       "0          1000  [672.5870361328125, 397.4309997558594, 2, 692....   \n",
       "1          1000  [1136.07421875, 266.266845703125, 2, 1155.6085...   \n",
       "2          1001  [682.8905029296875, 407.08642578125, 2, 702.61...   \n",
       "3          1001  [1134.0335693359375, 257.7659912109375, 2, 115...   \n",
       "4          1002  [691.739990234375, 412.57452392578125, 2, 713....   \n",
       "...         ...                                                ...   \n",
       "45401     46163  [704.8864135742188, 428.4450988769531, 2, 708....   \n",
       "45402     46163  [242.5023956298828, 475.6257019042969, 2, 245....   \n",
       "45403     46163  [1689.3052978515625, 480.3915100097656, 2, 168...   \n",
       "45404     46163  [1490.57373046875, 472.3777160644531, 2, 1493....   \n",
       "45405     46163  [1005.9852294921875, 454.8758239746094, 2, 100...   \n",
       "\n",
       "                                                     box  idx     score  \\\n",
       "0      [455.3162841796875, 340.884765625, 402.0437011...    1  0.898572   \n",
       "1      [932.1016845703125, 190.0831756591797, 310.273...    2  0.877560   \n",
       "2      [456.44172066815923, 337.55727539062497, 405.4...    1  0.891666   \n",
       "3      [938.4007471361167, 194.7648895263672, 306.255...    2  0.867543   \n",
       "4      [443.5269343344591, 336.3762005969505, 431.860...    1  0.859866   \n",
       "...                                                  ...  ...       ...   \n",
       "45401  [604.3679742712545, 384.44794226840435, 129.43...   29  0.775203   \n",
       "45402  [219.46201027790286, 441.22925448208105, 108.8...   12  0.555517   \n",
       "45403  [1669.4820232953628, 467.83948263581397, 20.96...   37  0.275808   \n",
       "45404  [1481.6382385788675, 450.34171338820823, 32.94...   54  0.676191   \n",
       "45405  [956.6008190511593, 409.1107207819066, 84.5862...   63  0.826677   \n",
       "\n",
       "       category_id  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "45401            1  \n",
       "45402            1  \n",
       "45403            1  \n",
       "45404            1  \n",
       "45405            1  \n",
       "\n",
       "[45406 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_pe_alphapose = customutils.load_pe_dataframe('alphapose')\n",
    "df_pe_openpose = customutils.load_pe_dataframe('openpose')\n",
    "df_pe_ViTPose = customutils.load_pe_dataframe('ViTPose')\n",
    "\n",
    "df_pe_alphapose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Athletics_Mixed_Tokyo_2020_20_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1920, 1080) to (1920, 1088) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ./videos/results/cotracker/Athletics_Mixed_Tokyo_2020_20_1.mp4\n",
      "Processing Athletics_Mixed_Tokyo_2020_39.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1920, 1080) to (1920, 1088) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ./videos/results/cotracker/Athletics_Mixed_Tokyo_2020_39.mp4\n",
      "Processing Athletics_Mixed_Tokyo_2020_5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1920, 1080) to (1920, 1088) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename)\n\u001b[1;32m      9\u001b[0m         video \u001b[38;5;241m=\u001b[39m load_video(filename)\n\u001b[0;32m---> 10\u001b[0m         frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mrun_cotracker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malphapose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_pe_alphapose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m customutils\u001b[38;5;241m.\u001b[39mwriteJson(frames,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideos/results/cotracker/person_keypoints_running.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mrun_cotracker\u001b[0;34m(filename, video, model_name, df_pe)\u001b[0m\n\u001b[1;32m     17\u001b[0m queries, queries_dict \u001b[38;5;241m=\u001b[39m get_queries_from_pe(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, df, df_annotations, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#queries = get_queries_for_frames(0, 1, annotations)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m pred_tracks, pred_visibility, indices \u001b[38;5;241m=\u001b[39m \u001b[43mcotracker_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m pred_tracks\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     23\u001b[0m np_pred \u001b[38;5;241m=\u001b[39m pred_tracks\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[4], line 106\u001b[0m, in \u001b[0;36mcotracker_model\u001b[0;34m(video, filename, model_name, person_count, queries, queries_dict, visualize)\u001b[0m\n\u001b[1;32m    104\u001b[0m     vis \u001b[38;5;241m=\u001b[39m Visualizer(save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./videos/results/cotracker/\u001b[39m\u001b[38;5;124m\"\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m#vis.visualize(video, pred_tracks, pred_visibility, filename = model_name + '_' + filename.replace('.mp4', ''), query_frame=0)\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pred_tracks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pred_visibility\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39munsqueeze(new_pred_tracks, \u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39munsqueeze(new_pred_visibility, \u001b[38;5;241m0\u001b[39m), indices\n",
      "File \u001b[0;32m/mnt/d/Justine/uni/co-tracker/cotracker/utils/visualizer.py:127\u001b[0m, in \u001b[0;36mVisualizer.visualize\u001b[0;34m(self, video, tracks, visibility, gt_tracks, segm_mask, filename, writer, step, query_frame, save_video, compensate_for_camera_motion)\u001b[0m\n\u001b[1;32m    117\u001b[0m res_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_tracks_on_video(\n\u001b[1;32m    118\u001b[0m     video\u001b[38;5;241m=\u001b[39mvideo,\n\u001b[1;32m    119\u001b[0m     tracks\u001b[38;5;241m=\u001b[39mtracks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     compensate_for_camera_motion\u001b[38;5;241m=\u001b[39mcompensate_for_camera_motion,\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_video:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_video\n",
      "File \u001b[0;32m/mnt/d/Justine/uni/co-tracker/cotracker/utils/visualizer.py:151\u001b[0m, in \u001b[0;36mVisualizer.save_video\u001b[0;34m(self, video, filename, writer, step)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Write frames to the video file\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m wide_list[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mvideo_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m video_writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/imageio/core/format.py:590\u001b[0m, in \u001b[0;36mFormat.Writer.append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    588\u001b[0m im \u001b[38;5;241m=\u001b[39m asarray(im)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Call\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/imageio/plugins/ffmpeg.py:600\u001b[0m, in \u001b[0;36mFfmpegFormat.Writer._append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_gen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Check status\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# Write. Yes, we can send the data in as a numpy array\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/py310_2/lib/python3.10/site-packages/imageio_ffmpeg/_io.py:627\u001b[0m, in \u001b[0;36mwrite_frames\u001b[0;34m(path, size, pix_fmt_in, pix_fmt_out, fps, quality, bitrate, codec, macro_block_size, ffmpeg_log_level, ffmpeg_timeout, input_params, output_params, audio_path, audio_codec)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# framesize = size[0] * size[1] * depth * bpp\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# assert isinstance(bb, bytes), \"Frame must be send as bytes\"\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# assert len(bb) == framesize, \"Frame must have width*height*depth*bpp bytes\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m \n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# Write\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 627\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# Show the command and stderr from pipe\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0:}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFFMPEG COMMAND:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{1:}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFFMPEG STDERR \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(err, cmd_str)\n\u001b[1;32m    633\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "for filename in filenames:\n",
    "    if filename == \"Athletics_Mixed_Tokyo_2020_64.mp4\" or filename == \"Triathlon_Men_Tokyo_2020_19_1.mp4\":\n",
    "        continue\n",
    "    isExist = os.path.exists(\"./videos/results/cotracker/\" + filename)\n",
    "    if not isExist:    \n",
    "        print(\"Processing \" + filename)\n",
    "        video = load_video(filename)\n",
    "        frames += run_cotracker(filename, video, 'alphapose', df_pe_alphapose)\n",
    "\n",
    "    \n",
    "customutils.writeJson(frames,'videos/results/cotracker/person_keypoints_running.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py390",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
